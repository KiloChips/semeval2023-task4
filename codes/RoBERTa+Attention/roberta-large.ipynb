{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:15.900032Z",
     "iopub.status.busy": "2023-01-13T11:44:15.899640Z",
     "iopub.status.idle": "2023-01-13T11:44:16.888879Z",
     "shell.execute_reply": "2023-01-13T11:44:16.887771Z",
     "shell.execute_reply.started": "2023-01-13T11:44:15.899996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 13 11:44:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:18.878932Z",
     "iopub.status.busy": "2023-01-13T11:44:18.878534Z",
     "iopub.status.idle": "2023-01-13T11:44:29.928570Z",
     "shell.execute_reply": "2023-01-13T11:44:29.927196Z",
     "shell.execute_reply.started": "2023-01-13T11:44:18.878894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:37.186644Z",
     "iopub.status.busy": "2023-01-13T11:44:37.186252Z",
     "iopub.status.idle": "2023-01-13T11:44:51.559355Z",
     "shell.execute_reply": "2023-01-13T11:44:51.558195Z",
     "shell.execute_reply.started": "2023-01-13T11:44:37.186612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3c4595e7d458687e80e86d93248fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0330756c91e04788bd759e15a4b8f6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ae036acdd94d6687862c6d4eac9f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb663cbc6efc4f23aaf7034685194130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', 128)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "\n",
    "# 参数设置\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "\n",
    "        self.SEED = 71\n",
    "        self.MODEL_PATH = 'roberta-large'\n",
    "        self.NUM_LABELS = 20\n",
    "\n",
    "        # data\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH = 128\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.VALIDATION_SPLIT = 0.10\n",
    "\n",
    "        # model\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 3e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        self.N_VALIDATE_DUR_TRAIN = 3\n",
    "        self.N_WARMUP = 0\n",
    "        self.SAVE_BEST_ONLY = True\n",
    "        self.EPOCHS = 15\n",
    "        self.USE_FGM = False\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(config.SEED)\n",
    "seed_torch(seed=config.SEED)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:51.562505Z",
     "iopub.status.busy": "2023-01-13T11:44:51.561522Z",
     "iopub.status.idle": "2023-01-13T11:44:51.567112Z",
     "shell.execute_reply": "2023-01-13T11:44:51.566027Z",
     "shell.execute_reply.started": "2023-01-13T11:44:51.562465Z"
    }
   },
   "outputs": [],
   "source": [
    "datadir = \"/kaggle/input/valueaaa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:51.569895Z",
     "iopub.status.busy": "2023-01-13T11:44:51.569012Z",
     "iopub.status.idle": "2023-01-13T11:44:51.579603Z",
     "shell.execute_reply": "2023-01-13T11:44:51.578769Z",
     "shell.execute_reply.started": "2023-01-13T11:44:51.569858Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def argumentadd(filepathArg,filepathLal):\n",
    "    trainArg = pd.read_csv(filepathArg, sep='\\t', header=0)\n",
    "    trainLal = pd.read_csv(filepathLal, sep='\\t', header=0)\n",
    "    def dataframetoList(X):\n",
    "        train_data = np.array(X)#np.ndarray()\n",
    "        train_x_list=train_data.tolist()#list\n",
    "        return train_x_list\n",
    "    \n",
    "    trainArgList = dataframetoList(trainArg)\n",
    "    \n",
    "    def insertSentence(dataList):\n",
    "        for x in range(0, len(dataList)):\n",
    "            sentence = \"Imagine someone is arguing \" + str(dataList[x][2]) + \" \\\"\" + str(dataList[x][1]) + \"\\\" \" + \"by saying: \\\"\" + str(dataList[x][3]) + \"\\\".\"\n",
    "            dataList[x].append(sentence)\n",
    "        return dataList\n",
    "    trainArgs = insertSentence(trainArgList)\n",
    "    \n",
    "    # list 转dataframe\n",
    "    trainArgD = pd.DataFrame(trainArgs)\n",
    "    trainArgD.columns = [\"Argument ID\",\"Conclusion\",\"Stance\",\"Premise\",\"sentence\"]\n",
    "    \n",
    "    trainArgD.drop(columns=[\"Conclusion\",\"Stance\",\"Premise\"],inplace=True)\n",
    "    train_merge = trainArgD.merge(trainLal,on='Argument ID')\n",
    "    \n",
    "    return train_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:10:13.898970Z",
     "iopub.status.busy": "2023-01-13T14:10:13.898590Z",
     "iopub.status.idle": "2023-01-13T14:10:14.012057Z",
     "shell.execute_reply": "2023-01-13T14:10:14.011014Z",
     "shell.execute_reply.started": "2023-01-13T14:10:13.898938Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = argumentadd(datadir+'/arguments-training.tsv',datadir + '/labels-training.tsv')\n",
    "valid_df = argumentadd(datadir+'/arguments-validation.tsv',datadir + '/labels-validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:44:58.210417Z",
     "iopub.status.busy": "2023-01-13T11:44:58.209721Z",
     "iopub.status.idle": "2023-01-13T11:44:58.231369Z",
     "shell.execute_reply": "2023-01-13T11:44:58.229393Z",
     "shell.execute_reply.started": "2023-01-13T11:44:58.210367Z"
    }
   },
   "outputs": [],
   "source": [
    "def testcreate(filepathArg):\n",
    "    test = pd.read_csv(filepathArg, sep='\\t', header=0)\n",
    "    def dataframetoList(X):\n",
    "        train_data = np.array(X)#np.ndarray()\n",
    "        train_x_list=train_data.tolist()#list\n",
    "        return train_x_list\n",
    "    def insertSentence(dataList):\n",
    "        for x in range(0, len(dataList)):\n",
    "            sentence = \"Imagine someone is arguing \" + str(dataList[x][2]) + \" \\\"\" + str(dataList[x][1]) + \"\\\" \" + \"by saying: \\\"\" + str(dataList[x][3]) + \"\\\".\"\n",
    "            dataList[x].append(sentence)\n",
    "        return dataList\n",
    "    \n",
    "    testList = dataframetoList(test)\n",
    "    testLists = insertSentence(testList)\n",
    "    testD = pd.DataFrame(testLists)\n",
    "    testD.columns = [\"Argument ID\",\"Conclusion\",\"Stance\",\"Premise\",\"sentence\"]\n",
    "    testD.drop(columns=[\"Conclusion\",\"Stance\",\"Premise\"],inplace=True)\n",
    "    return testD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:13.308361Z",
     "iopub.status.busy": "2023-01-13T11:45:13.307993Z",
     "iopub.status.idle": "2023-01-13T11:45:13.342998Z",
     "shell.execute_reply": "2023-01-13T11:45:13.342121Z",
     "shell.execute_reply.started": "2023-01-13T11:45:13.308332Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = testcreate(datadir+'/arguments-test.tsv')\n",
    "test2_df = testcreate(datadir+'/arguments-test-nahjalbalagha.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:18.055807Z",
     "iopub.status.busy": "2023-01-13T11:45:18.055127Z",
     "iopub.status.idle": "2023-01-13T11:45:18.065058Z",
     "shell.execute_reply": "2023-01-13T11:45:18.063992Z",
     "shell.execute_reply.started": "2023-01-13T11:45:18.055770Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, df, set_type=None):\n",
    "        super(TransformerDataset, self).__init__()\n",
    "\n",
    "        #df = df.iloc[indices]\n",
    "        self.texts = df['sentence'].values.tolist()\n",
    "        self.set_type = set_type\n",
    "        if self.set_type != 'test':\n",
    "            self.labels = df.iloc[:, 2:].values\n",
    "\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_length = config.MAX_LENGTH\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            self.texts[index],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = tokenized['input_ids'].squeeze()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "\n",
    "        if self.set_type != 'test':\n",
    "            return {\n",
    "                'input_ids': input_ids.long(),\n",
    "                'attention_mask': attention_mask.long(),\n",
    "                'labels': torch.Tensor(self.labels[index]).float(),\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.long(),\n",
    "            'attention_mask': attention_mask.long(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:20.637712Z",
     "iopub.status.busy": "2023-01-13T11:45:20.637351Z",
     "iopub.status.idle": "2023-01-13T11:45:20.645261Z",
     "shell.execute_reply": "2023-01-13T11:45:20.644216Z",
     "shell.execute_reply.started": "2023-01-13T11:45:20.637682Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = TransformerDataset(train_df)\n",
    "valid_data = TransformerDataset(valid_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:22.901639Z",
     "iopub.status.busy": "2023-01-13T11:45:22.901280Z",
     "iopub.status.idle": "2023-01-13T11:45:22.911550Z",
     "shell.execute_reply": "2023-01-13T11:45:22.909491Z",
     "shell.execute_reply.started": "2023-01-13T11:45:22.901608Z"
    }
   },
   "outputs": [],
   "source": [
    "class FGM(object):\n",
    "    def __init__(self, model, emb_name, epsilon=1.0):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.emb_name = emb_name\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:25.420959Z",
     "iopub.status.busy": "2023-01-13T11:45:25.420331Z",
     "iopub.status.idle": "2023-01-13T11:45:25.437025Z",
     "shell.execute_reply": "2023-01-13T11:45:25.436006Z",
     "shell.execute_reply.started": "2023-01-13T11:45:25.420849Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:47:27.149466Z",
     "iopub.status.busy": "2023-01-13T11:47:27.149089Z",
     "iopub.status.idle": "2023-01-13T11:47:27.175293Z",
     "shell.execute_reply": "2023-01-13T11:47:27.174131Z",
     "shell.execute_reply.started": "2023-01-13T11:47:27.149436Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        cfg = AutoConfig.from_pretrained(config.MODEL_PATH)\n",
    "        cfg.update({\"output_hidden_states\": True,\n",
    "                    \"hidden_dropout_prob\": 0.0,\n",
    "                    \"layer_norm_eps\": 1e-7})\n",
    "\n",
    "        self.roberta = AutoModel.from_pretrained(config.MODEL_PATH, config=cfg)\n",
    "\n",
    "        dim = self.roberta.pooler.dense.bias.shape[0]\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim, 20)\n",
    "        )\n",
    "        init_params([self.cls, self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in roberta_output[2][-24:]], dim=0\n",
    "        )\n",
    "        # print(\"处理前：\",cls_outputs.shape)\n",
    "        cls_output = (\n",
    "                torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(\n",
    "            0)\n",
    "        # print(\"处理后：\",cls_outputs.shape)\n",
    "        # print(type(cls_outputs))\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        #print(\"logits:\",logits.shape)\n",
    "        return self.cls(logits)\n",
    "    \n",
    "device = config.DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:39.044996Z",
     "iopub.status.busy": "2023-01-13T11:45:39.044630Z",
     "iopub.status.idle": "2023-01-13T11:45:48.558468Z",
     "shell.execute_reply": "2023-01-13T11:45:48.557173Z",
     "shell.execute_reply.started": "2023-01-13T11:45:39.044963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (22.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:50.260007Z",
     "iopub.status.busy": "2023-01-13T11:45:50.259614Z",
     "iopub.status.idle": "2023-01-13T11:45:55.733805Z",
     "shell.execute_reply": "2023-01-13T11:45:55.732191Z",
     "shell.execute_reply.started": "2023-01-13T11:45:50.259969Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MultilabelF1Score\n",
    "metric = MultilabelF1Score(num_labels=20,average=\"macro\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:45:58.127986Z",
     "iopub.status.busy": "2023-01-13T11:45:58.127545Z",
     "iopub.status.idle": "2023-01-13T11:45:58.142929Z",
     "shell.execute_reply": "2023-01-13T11:45:58.141653Z",
     "shell.execute_reply.started": "2023-01-13T11:45:58.127943Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def val(model, valid_dataloader, criterion, metric):\n",
    "    val_loss = 0\n",
    "    true, pred = [], []\n",
    "\n",
    "    # set model.eval() every time during evaluation\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "            probs = F.sigmoid(logits)\n",
    "            metric.update(probs, b_labels)\n",
    "\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(logits, b_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_dataloader)\n",
    "    f1_score= float(metric.compute())\n",
    "    print(\"eval loss: %.5f, f1 score: %.5f\" %\n",
    "          (avg_val_loss, f1_score))\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:46:03.293694Z",
     "iopub.status.busy": "2023-01-13T11:46:03.293274Z",
     "iopub.status.idle": "2023-01-13T11:46:03.308538Z",
     "shell.execute_reply": "2023-01-13T11:46:03.307484Z",
     "shell.execute_reply.started": "2023-01-13T11:46:03.293662Z"
    }
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # setting a seed ensures reproducible results.\n",
    "    # seed may affect the performance too.\n",
    "    torch.manual_seed(config.SEED)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = RMSELoss()\n",
    "\n",
    "    # define the parameters to be optmized -\n",
    "    # - and add regularization\n",
    "    if config.FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
    "\n",
    "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    max_f1_score = 0\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        #best_model, best_val_mse_score = train(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, epoch, metric)\n",
    "        f1_score = val(model, valid_dataloader, criterion, metric)\n",
    "        if config.USE_FGM:\n",
    "          fgm = FGM(model, epsilon=1, emb_name='word_embeddings.')\n",
    "\n",
    "        #train_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader,\n",
    "                                          desc='Epoch ' + str(epoch))):\n",
    "            # set model.eval() every time during training\n",
    "            model.train()\n",
    "\n",
    "            # unpack the batch contents and push them to the device (cuda or cpu).\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attention_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            # clear accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "            probs = F.sigmoid(logits)\n",
    "            metric.update(probs, b_labels)\n",
    "            f1_score= float(metric.compute())\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(logits, b_labels)\n",
    "            #train_loss += loss.item()\n",
    "            if step % 10 == 0:\n",
    "                print(\n",
    "                    \"step %d, loss: %.5f, f1 score: %.5f\"\n",
    "                    % (step, loss, f1_score,\n",
    "                        ))\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # fgm attack\n",
    "            if config.USE_FGM:\n",
    "                fgm.attack()\n",
    "                logits_adv = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "                loss_adv = criterion(logits_adv, b_labels)\n",
    "                loss_adv.backward()\n",
    "                fgm.restore()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # update scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            mertic1 = MultilabelF1Score(num_labels=20,average=\"macro\").to(device)\n",
    "            if step % 40 == 0:\n",
    "                print(f'-- Step: {step}')\n",
    "                f1_score = val(model, valid_dataloader, criterion, mertic1)\n",
    "\n",
    "                # min_f1_score = float('inf')\n",
    "                if config.SAVE_BEST_ONLY:\n",
    "                    if f1_score > max_f1_score:\n",
    "                        best_model = copy.deepcopy(model)\n",
    "                        best_val_mse_score = f1_score\n",
    "\n",
    "                        model_name = 'roberta_best_model'\n",
    "                        torch.save(best_model.state_dict(), model_name + '.pt')\n",
    "\n",
    "                        print(f'--- Best Model. Val loss: {max_f1_score} -> {f1_score}')\n",
    "                        max_f1_score = f1_score\n",
    "\n",
    "        #avg_train_loss = train_loss / len(train_dataloader)\n",
    "        #print('Training loss:', avg_train_loss)\n",
    "        #print(\"Training loss:: %.5f, f1 score: %.5f\" %\n",
    "              #(avg_train_loss, f1_score))\n",
    "\n",
    "\n",
    "    return best_model, best_val_mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:47:39.793638Z",
     "iopub.status.busy": "2023-01-13T11:47:39.793259Z",
     "iopub.status.idle": "2023-01-13T11:47:44.947931Z",
     "shell.execute_reply": "2023-01-13T11:47:44.946990Z",
     "shell.execute_reply.started": "2023-01-13T11:47:39.793608Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (high_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (attention): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       "  (cls): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T11:47:50.618134Z",
     "iopub.status.busy": "2023-01-13T11:47:50.617760Z",
     "iopub.status.idle": "2023-01-13T14:00:01.934206Z",
     "shell.execute_reply": "2023-01-13T14:00:01.933136Z",
     "shell.execute_reply.started": "2023-01-13T11:47:50.618100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.89020, f1 score: 0.15596\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bed9d1dcbd4781b47f61ae0686cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.91733, f1 score: 0.15609\n",
      "-- Step: 0\n",
      "eval loss: 0.71285, f1 score: 0.14105\n",
      "--- Best Model. Val loss: 0 -> 0.14104756712913513\n",
      "step 10, loss: 0.41007, f1 score: 0.15518\n",
      "step 20, loss: 0.41795, f1 score: 0.15276\n",
      "step 30, loss: 0.38134, f1 score: 0.15121\n",
      "step 40, loss: 0.41716, f1 score: 0.14921\n",
      "-- Step: 40\n",
      "eval loss: 0.41627, f1 score: 0.01854\n",
      "step 50, loss: 0.46347, f1 score: 0.14785\n",
      "step 60, loss: 0.40921, f1 score: 0.14620\n",
      "step 70, loss: 0.37673, f1 score: 0.14804\n",
      "step 80, loss: 0.42127, f1 score: 0.14877\n",
      "-- Step: 80\n",
      "eval loss: 0.40277, f1 score: 0.10422\n",
      "step 90, loss: 0.32208, f1 score: 0.15121\n",
      "step 100, loss: 0.32840, f1 score: 0.15243\n",
      "step 110, loss: 0.40268, f1 score: 0.15453\n",
      "step 120, loss: 0.34662, f1 score: 0.15666\n",
      "-- Step: 120\n",
      "eval loss: 0.37806, f1 score: 0.14914\n",
      "--- Best Model. Val loss: 0.14104756712913513 -> 0.14914363622665405\n",
      "step 130, loss: 0.37443, f1 score: 0.16002\n",
      "step 140, loss: 0.33630, f1 score: 0.16783\n",
      "step 150, loss: 0.37870, f1 score: 0.17319\n",
      "step 160, loss: 0.33131, f1 score: 0.17889\n",
      "-- Step: 160\n",
      "eval loss: 0.35711, f1 score: 0.25863\n",
      "--- Best Model. Val loss: 0.14914363622665405 -> 0.25862637162208557\n",
      "step 170, loss: 0.33370, f1 score: 0.18631\n",
      "step 180, loss: 0.33759, f1 score: 0.19109\n",
      "step 190, loss: 0.33200, f1 score: 0.19620\n",
      "step 200, loss: 0.29755, f1 score: 0.20066\n",
      "-- Step: 200\n",
      "eval loss: 0.34238, f1 score: 0.28238\n",
      "--- Best Model. Val loss: 0.25862637162208557 -> 0.282376229763031\n",
      "step 210, loss: 0.37218, f1 score: 0.20658\n",
      "step 220, loss: 0.34782, f1 score: 0.21079\n",
      "step 230, loss: 0.28295, f1 score: 0.21527\n",
      "step 240, loss: 0.23032, f1 score: 0.22073\n",
      "-- Step: 240\n",
      "eval loss: 0.32711, f1 score: 0.33516\n",
      "--- Best Model. Val loss: 0.282376229763031 -> 0.3351631462574005\n",
      "step 250, loss: 0.28711, f1 score: 0.22428\n",
      "step 260, loss: 0.30810, f1 score: 0.22771\n",
      "step 270, loss: 0.34986, f1 score: 0.23206\n",
      "step 280, loss: 0.31528, f1 score: 0.23485\n",
      "-- Step: 280\n",
      "eval loss: 0.32944, f1 score: 0.32664\n",
      "step 290, loss: 0.29306, f1 score: 0.23730\n",
      "step 300, loss: 0.32500, f1 score: 0.24055\n",
      "step 310, loss: 0.30222, f1 score: 0.24445\n",
      "step 320, loss: 0.28819, f1 score: 0.24765\n",
      "-- Step: 320\n",
      "eval loss: 0.32228, f1 score: 0.34749\n",
      "--- Best Model. Val loss: 0.3351631462574005 -> 0.34748539328575134\n",
      "step 330, loss: 0.30490, f1 score: 0.25076\n",
      "eval loss: 0.31852, f1 score: 0.35749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c36f7d7a4342a6beeb4d4fc9105636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.29046, f1 score: 0.35843\n",
      "-- Step: 0\n",
      "eval loss: 0.32084, f1 score: 0.36175\n",
      "--- Best Model. Val loss: 0.34748539328575134 -> 0.36175423860549927\n",
      "step 10, loss: 0.24900, f1 score: 0.37386\n",
      "step 20, loss: 0.25318, f1 score: 0.39078\n",
      "step 30, loss: 0.31662, f1 score: 0.40585\n",
      "step 40, loss: 0.33044, f1 score: 0.41554\n",
      "-- Step: 40\n",
      "eval loss: 0.32422, f1 score: 0.37590\n",
      "--- Best Model. Val loss: 0.36175423860549927 -> 0.3759002089500427\n",
      "step 50, loss: 0.25570, f1 score: 0.42169\n",
      "step 60, loss: 0.29474, f1 score: 0.42810\n",
      "step 70, loss: 0.25612, f1 score: 0.43196\n",
      "step 80, loss: 0.35485, f1 score: 0.43786\n",
      "-- Step: 80\n",
      "eval loss: 0.32439, f1 score: 0.35122\n",
      "step 90, loss: 0.20908, f1 score: 0.44406\n",
      "step 100, loss: 0.27556, f1 score: 0.44705\n",
      "step 110, loss: 0.25028, f1 score: 0.44998\n",
      "step 120, loss: 0.33831, f1 score: 0.45392\n",
      "-- Step: 120\n",
      "eval loss: 0.32188, f1 score: 0.39759\n",
      "--- Best Model. Val loss: 0.3759002089500427 -> 0.39759236574172974\n",
      "step 130, loss: 0.32564, f1 score: 0.45792\n",
      "step 140, loss: 0.23363, f1 score: 0.46132\n",
      "step 150, loss: 0.23132, f1 score: 0.46384\n",
      "step 160, loss: 0.26618, f1 score: 0.46662\n",
      "-- Step: 160\n",
      "eval loss: 0.32115, f1 score: 0.39371\n",
      "step 170, loss: 0.22389, f1 score: 0.47021\n",
      "step 180, loss: 0.31715, f1 score: 0.47252\n",
      "step 190, loss: 0.31687, f1 score: 0.47500\n",
      "step 200, loss: 0.34694, f1 score: 0.47538\n",
      "-- Step: 200\n",
      "eval loss: 0.32014, f1 score: 0.39858\n",
      "--- Best Model. Val loss: 0.39759236574172974 -> 0.3985787034034729\n",
      "step 210, loss: 0.27078, f1 score: 0.47825\n",
      "step 220, loss: 0.21577, f1 score: 0.47984\n",
      "step 230, loss: 0.21577, f1 score: 0.48074\n",
      "step 240, loss: 0.32319, f1 score: 0.48340\n",
      "-- Step: 240\n",
      "eval loss: 0.31859, f1 score: 0.40906\n",
      "--- Best Model. Val loss: 0.3985787034034729 -> 0.4090595245361328\n",
      "step 250, loss: 0.28984, f1 score: 0.48430\n",
      "step 260, loss: 0.19708, f1 score: 0.48700\n",
      "step 270, loss: 0.20644, f1 score: 0.48779\n",
      "step 280, loss: 0.24238, f1 score: 0.48923\n",
      "-- Step: 280\n",
      "eval loss: 0.32443, f1 score: 0.41577\n",
      "--- Best Model. Val loss: 0.4090595245361328 -> 0.4157654941082001\n",
      "step 290, loss: 0.27876, f1 score: 0.49276\n",
      "step 300, loss: 0.25096, f1 score: 0.49458\n",
      "step 310, loss: 0.21028, f1 score: 0.49521\n",
      "step 320, loss: 0.30496, f1 score: 0.49609\n",
      "-- Step: 320\n",
      "eval loss: 0.31494, f1 score: 0.37313\n",
      "step 330, loss: 0.23053, f1 score: 0.49653\n",
      "eval loss: 0.31397, f1 score: 0.41194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e998843e3d4f97811cd9dbfde6b34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.18459, f1 score: 0.41578\n",
      "-- Step: 0\n",
      "eval loss: 0.31473, f1 score: 0.40712\n",
      "step 10, loss: 0.18136, f1 score: 0.43698\n",
      "step 20, loss: 0.23251, f1 score: 0.46293\n",
      "step 30, loss: 0.23692, f1 score: 0.48022\n",
      "step 40, loss: 0.16865, f1 score: 0.49409\n",
      "-- Step: 40\n",
      "eval loss: 0.33129, f1 score: 0.41896\n",
      "--- Best Model. Val loss: 0.4157654941082001 -> 0.41895946860313416\n",
      "step 50, loss: 0.19100, f1 score: 0.50841\n",
      "step 60, loss: 0.19709, f1 score: 0.51919\n",
      "step 70, loss: 0.20970, f1 score: 0.53069\n",
      "step 80, loss: 0.21838, f1 score: 0.53894\n",
      "-- Step: 80\n",
      "eval loss: 0.33198, f1 score: 0.43147\n",
      "--- Best Model. Val loss: 0.41895946860313416 -> 0.43147149682044983\n",
      "step 90, loss: 0.20255, f1 score: 0.54686\n",
      "step 100, loss: 0.20057, f1 score: 0.55392\n",
      "step 110, loss: 0.19049, f1 score: 0.56257\n",
      "step 120, loss: 0.21821, f1 score: 0.56832\n",
      "-- Step: 120\n",
      "eval loss: 0.33818, f1 score: 0.43392\n",
      "--- Best Model. Val loss: 0.43147149682044983 -> 0.43392378091812134\n",
      "step 130, loss: 0.19203, f1 score: 0.57593\n",
      "step 140, loss: 0.22013, f1 score: 0.58244\n",
      "step 150, loss: 0.21365, f1 score: 0.58617\n",
      "step 160, loss: 0.19336, f1 score: 0.58911\n",
      "-- Step: 160\n",
      "eval loss: 0.33060, f1 score: 0.42700\n",
      "step 170, loss: 0.15683, f1 score: 0.59215\n",
      "step 180, loss: 0.16472, f1 score: 0.59582\n",
      "step 190, loss: 0.19407, f1 score: 0.59621\n",
      "step 200, loss: 0.15615, f1 score: 0.59745\n",
      "-- Step: 200\n",
      "eval loss: 0.33142, f1 score: 0.42087\n",
      "step 210, loss: 0.24773, f1 score: 0.59862\n",
      "step 220, loss: 0.15483, f1 score: 0.60041\n",
      "step 230, loss: 0.26356, f1 score: 0.60184\n",
      "step 240, loss: 0.17872, f1 score: 0.60416\n",
      "-- Step: 240\n",
      "eval loss: 0.33405, f1 score: 0.42891\n",
      "step 250, loss: 0.19004, f1 score: 0.60453\n",
      "step 260, loss: 0.24539, f1 score: 0.60518\n",
      "step 270, loss: 0.24770, f1 score: 0.60636\n",
      "step 280, loss: 0.22943, f1 score: 0.60618\n",
      "-- Step: 280\n",
      "eval loss: 0.33266, f1 score: 0.44134\n",
      "--- Best Model. Val loss: 0.43392378091812134 -> 0.4413423538208008\n",
      "step 290, loss: 0.21817, f1 score: 0.60738\n",
      "step 300, loss: 0.19716, f1 score: 0.60812\n",
      "step 310, loss: 0.23311, f1 score: 0.60840\n",
      "step 320, loss: 0.23063, f1 score: 0.60964\n",
      "-- Step: 320\n",
      "eval loss: 0.33190, f1 score: 0.43737\n",
      "step 330, loss: 0.21141, f1 score: 0.61078\n",
      "eval loss: 0.32405, f1 score: 0.43883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c82ab559d134712a1eca23f689ac94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.13455, f1 score: 0.44236\n",
      "-- Step: 0\n",
      "eval loss: 0.32536, f1 score: 0.43890\n",
      "step 10, loss: 0.16423, f1 score: 0.47546\n",
      "step 20, loss: 0.17136, f1 score: 0.50069\n",
      "step 30, loss: 0.16259, f1 score: 0.52551\n",
      "step 40, loss: 0.11384, f1 score: 0.54957\n",
      "-- Step: 40\n",
      "eval loss: 0.34778, f1 score: 0.44565\n",
      "--- Best Model. Val loss: 0.4413423538208008 -> 0.4456491768360138\n",
      "step 50, loss: 0.13056, f1 score: 0.56458\n",
      "step 60, loss: 0.12335, f1 score: 0.58198\n",
      "step 70, loss: 0.10494, f1 score: 0.59391\n",
      "step 80, loss: 0.15239, f1 score: 0.60262\n",
      "-- Step: 80\n",
      "eval loss: 0.34941, f1 score: 0.44837\n",
      "--- Best Model. Val loss: 0.4456491768360138 -> 0.4483668804168701\n",
      "step 90, loss: 0.14079, f1 score: 0.61423\n",
      "step 100, loss: 0.12297, f1 score: 0.62428\n",
      "step 110, loss: 0.12685, f1 score: 0.63244\n",
      "step 120, loss: 0.13678, f1 score: 0.63966\n",
      "-- Step: 120\n",
      "eval loss: 0.34855, f1 score: 0.43736\n",
      "step 130, loss: 0.13984, f1 score: 0.64658\n",
      "step 140, loss: 0.13155, f1 score: 0.65452\n",
      "step 150, loss: 0.10855, f1 score: 0.66121\n",
      "step 160, loss: 0.13999, f1 score: 0.66597\n",
      "-- Step: 160\n",
      "eval loss: 0.35217, f1 score: 0.45142\n",
      "--- Best Model. Val loss: 0.4483668804168701 -> 0.45142191648483276\n",
      "step 170, loss: 0.17293, f1 score: 0.67129\n",
      "step 180, loss: 0.13919, f1 score: 0.67418\n",
      "step 190, loss: 0.13568, f1 score: 0.67975\n",
      "step 200, loss: 0.16487, f1 score: 0.68315\n",
      "-- Step: 200\n",
      "eval loss: 0.35507, f1 score: 0.46150\n",
      "--- Best Model. Val loss: 0.45142191648483276 -> 0.46149685978889465\n",
      "step 210, loss: 0.12054, f1 score: 0.68847\n",
      "step 220, loss: 0.11465, f1 score: 0.69208\n",
      "step 230, loss: 0.10547, f1 score: 0.69460\n",
      "step 240, loss: 0.12297, f1 score: 0.69794\n",
      "-- Step: 240\n",
      "eval loss: 0.35444, f1 score: 0.47129\n",
      "--- Best Model. Val loss: 0.46149685978889465 -> 0.47128793597221375\n",
      "step 250, loss: 0.13748, f1 score: 0.70112\n",
      "step 260, loss: 0.13776, f1 score: 0.70332\n",
      "step 270, loss: 0.14465, f1 score: 0.70484\n",
      "step 280, loss: 0.15382, f1 score: 0.70737\n",
      "-- Step: 280\n",
      "eval loss: 0.35712, f1 score: 0.44128\n",
      "step 290, loss: 0.13593, f1 score: 0.70968\n",
      "step 300, loss: 0.12274, f1 score: 0.71080\n",
      "step 310, loss: 0.14904, f1 score: 0.71256\n",
      "step 320, loss: 0.13474, f1 score: 0.71395\n",
      "-- Step: 320\n",
      "eval loss: 0.36054, f1 score: 0.45401\n",
      "step 330, loss: 0.10491, f1 score: 0.71579\n",
      "eval loss: 0.35577, f1 score: 0.43453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37facd2751484b86a3f8f4d0e0b37337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.11704, f1 score: 0.43860\n",
      "-- Step: 0\n",
      "eval loss: 0.35724, f1 score: 0.43345\n",
      "step 10, loss: 0.11249, f1 score: 0.48265\n",
      "step 20, loss: 0.07821, f1 score: 0.51974\n",
      "step 30, loss: 0.09694, f1 score: 0.55179\n",
      "step 40, loss: 0.13900, f1 score: 0.57610\n",
      "-- Step: 40\n",
      "eval loss: 0.36949, f1 score: 0.43805\n",
      "step 50, loss: 0.07621, f1 score: 0.59893\n",
      "step 60, loss: 0.05234, f1 score: 0.62042\n",
      "step 70, loss: 0.06883, f1 score: 0.63794\n",
      "step 80, loss: 0.07157, f1 score: 0.65199\n",
      "-- Step: 80\n",
      "eval loss: 0.37524, f1 score: 0.44496\n",
      "step 90, loss: 0.07393, f1 score: 0.66413\n",
      "step 100, loss: 0.05685, f1 score: 0.67867\n",
      "step 110, loss: 0.07841, f1 score: 0.68937\n",
      "step 120, loss: 0.08787, f1 score: 0.70095\n",
      "-- Step: 120\n",
      "eval loss: 0.37641, f1 score: 0.45506\n",
      "step 130, loss: 0.07699, f1 score: 0.70880\n",
      "step 140, loss: 0.09155, f1 score: 0.71645\n",
      "step 150, loss: 0.09108, f1 score: 0.72533\n",
      "step 160, loss: 0.07624, f1 score: 0.73339\n",
      "-- Step: 160\n",
      "eval loss: 0.37969, f1 score: 0.44448\n",
      "step 180, loss: 0.07083, f1 score: 0.74468\n",
      "step 190, loss: 0.10064, f1 score: 0.74934\n",
      "eval loss: 0.38195, f1 score: 0.45375\n",
      "step 210, loss: 0.09261, f1 score: 0.75917\n",
      "step 220, loss: 0.12430, f1 score: 0.76334\n",
      "step 230, loss: 0.07025, f1 score: 0.76840\n",
      "step 240, loss: 0.08394, f1 score: 0.77291\n",
      "-- Step: 240\n",
      "eval loss: 0.38559, f1 score: 0.45195\n",
      "step 250, loss: 0.06778, f1 score: 0.77648\n",
      "step 260, loss: 0.04773, f1 score: 0.78032\n",
      "step 270, loss: 0.08639, f1 score: 0.78302\n",
      "step 280, loss: 0.06602, f1 score: 0.78621\n",
      "-- Step: 280\n",
      "eval loss: 0.38858, f1 score: 0.43706\n",
      "step 290, loss: 0.08219, f1 score: 0.78997\n",
      "step 300, loss: 0.10706, f1 score: 0.79257\n",
      "step 310, loss: 0.05427, f1 score: 0.79485\n",
      "step 320, loss: 0.06071, f1 score: 0.79698\n",
      "-- Step: 320\n",
      "eval loss: 0.38901, f1 score: 0.47246\n",
      "--- Best Model. Val loss: 0.47128793597221375 -> 0.47245943546295166\n",
      "step 330, loss: 0.06375, f1 score: 0.80074\n",
      "eval loss: 0.39377, f1 score: 0.46725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b49974e364242679c8a0e9d437c9836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.04681, f1 score: 0.47438\n",
      "-- Step: 0\n",
      "eval loss: 0.39426, f1 score: 0.46597\n",
      "step 10, loss: 0.04885, f1 score: 0.52108\n",
      "step 20, loss: 0.03341, f1 score: 0.56238\n",
      "step 30, loss: 0.04193, f1 score: 0.59877\n",
      "step 40, loss: 0.03920, f1 score: 0.62872\n",
      "-- Step: 40\n",
      "eval loss: 0.39632, f1 score: 0.44631\n",
      "step 50, loss: 0.03493, f1 score: 0.65217\n",
      "step 60, loss: 0.03533, f1 score: 0.67333\n",
      "step 70, loss: 0.04236, f1 score: 0.69187\n",
      "step 80, loss: 0.08256, f1 score: 0.70623\n",
      "-- Step: 80\n",
      "eval loss: 0.40430, f1 score: 0.44124\n",
      "step 90, loss: 0.04322, f1 score: 0.72117\n",
      "step 100, loss: 0.02987, f1 score: 0.73305\n",
      "step 110, loss: 0.03967, f1 score: 0.74250\n",
      "step 120, loss: 0.05565, f1 score: 0.75144\n",
      "-- Step: 120\n",
      "eval loss: 0.40684, f1 score: 0.45063\n",
      "step 130, loss: 0.04089, f1 score: 0.75884\n",
      "step 140, loss: 0.03153, f1 score: 0.76701\n",
      "step 150, loss: 0.06294, f1 score: 0.77388\n",
      "step 160, loss: 0.02964, f1 score: 0.78194\n",
      "-- Step: 160\n",
      "eval loss: 0.41217, f1 score: 0.44909\n",
      "step 170, loss: 0.02797, f1 score: 0.78841\n",
      "step 180, loss: 0.04904, f1 score: 0.79436\n",
      "step 190, loss: 0.03108, f1 score: 0.80052\n",
      "step 200, loss: 0.05878, f1 score: 0.80613\n",
      "-- Step: 200\n",
      "eval loss: 0.41531, f1 score: 0.44717\n",
      "step 210, loss: 0.03527, f1 score: 0.81168\n",
      "step 220, loss: 0.02879, f1 score: 0.81658\n",
      "step 230, loss: 0.09250, f1 score: 0.82083\n",
      "step 240, loss: 0.02509, f1 score: 0.82513\n",
      "-- Step: 240\n",
      "eval loss: 0.41464, f1 score: 0.45371\n",
      "step 250, loss: 0.02444, f1 score: 0.82889\n",
      "step 260, loss: 0.06917, f1 score: 0.83221\n",
      "step 270, loss: 0.05462, f1 score: 0.83509\n",
      "step 280, loss: 0.03491, f1 score: 0.83861\n",
      "-- Step: 280\n",
      "eval loss: 0.41997, f1 score: 0.45770\n",
      "step 290, loss: 0.04901, f1 score: 0.84174\n",
      "step 300, loss: 0.06239, f1 score: 0.84501\n",
      "step 310, loss: 0.04406, f1 score: 0.84818\n",
      "step 320, loss: 0.02692, f1 score: 0.85009\n",
      "-- Step: 320\n",
      "eval loss: 0.41942, f1 score: 0.44983\n",
      "step 330, loss: 0.04290, f1 score: 0.85243\n",
      "eval loss: 0.42130, f1 score: 0.46324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb3fc9127664c3bb1cef261ff664272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.02324, f1 score: 0.46874\n",
      "-- Step: 0\n",
      "eval loss: 0.42239, f1 score: 0.46237\n",
      "step 10, loss: 0.05948, f1 score: 0.52339\n",
      "step 20, loss: 0.01519, f1 score: 0.56321\n",
      "step 30, loss: 0.02461, f1 score: 0.59923\n",
      "step 40, loss: 0.02067, f1 score: 0.62732\n",
      "-- Step: 40\n",
      "eval loss: 0.42726, f1 score: 0.45968\n",
      "step 50, loss: 0.01860, f1 score: 0.65210\n",
      "step 60, loss: 0.05125, f1 score: 0.67466\n",
      "step 70, loss: 0.04355, f1 score: 0.69199\n",
      "step 80, loss: 0.02386, f1 score: 0.70875\n",
      "-- Step: 80\n",
      "eval loss: 0.43180, f1 score: 0.45374\n",
      "step 90, loss: 0.04702, f1 score: 0.72298\n",
      "step 100, loss: 0.01568, f1 score: 0.73549\n",
      "step 110, loss: 0.02680, f1 score: 0.74911\n",
      "step 120, loss: 0.03397, f1 score: 0.76064\n",
      "-- Step: 120\n",
      "eval loss: 0.42946, f1 score: 0.46110\n",
      "step 130, loss: 0.01477, f1 score: 0.77148\n",
      "step 140, loss: 0.02616, f1 score: 0.78073\n",
      "step 150, loss: 0.02488, f1 score: 0.78966\n",
      "step 160, loss: 0.02217, f1 score: 0.79713\n",
      "-- Step: 160\n",
      "eval loss: 0.43649, f1 score: 0.44858\n",
      "step 170, loss: 0.02135, f1 score: 0.80392\n",
      "step 180, loss: 0.02073, f1 score: 0.81031\n",
      "step 190, loss: 0.02007, f1 score: 0.81694\n",
      "step 200, loss: 0.06904, f1 score: 0.82199\n",
      "-- Step: 200\n",
      "eval loss: 0.43353, f1 score: 0.45766\n",
      "step 210, loss: 0.02241, f1 score: 0.82654\n",
      "step 220, loss: 0.01326, f1 score: 0.83072\n",
      "step 230, loss: 0.02270, f1 score: 0.83529\n",
      "step 240, loss: 0.05718, f1 score: 0.83932\n",
      "-- Step: 240\n",
      "eval loss: 0.43888, f1 score: 0.45206\n",
      "step 250, loss: 0.03395, f1 score: 0.84334\n",
      "step 260, loss: 0.01722, f1 score: 0.84687\n",
      "step 270, loss: 0.02087, f1 score: 0.85081\n",
      "step 280, loss: 0.04950, f1 score: 0.85455\n",
      "-- Step: 280\n",
      "eval loss: 0.44118, f1 score: 0.45045\n",
      "step 290, loss: 0.02337, f1 score: 0.85753\n",
      "step 300, loss: 0.02580, f1 score: 0.86092\n",
      "step 310, loss: 0.01358, f1 score: 0.86343\n",
      "step 320, loss: 0.04849, f1 score: 0.86638\n",
      "-- Step: 320\n",
      "eval loss: 0.44690, f1 score: 0.44430\n",
      "step 330, loss: 0.03059, f1 score: 0.86878\n",
      "eval loss: 0.44454, f1 score: 0.45466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b533b3dbca44206afea90960de8e7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.02168, f1 score: 0.45812\n",
      "-- Step: 0\n",
      "eval loss: 0.44480, f1 score: 0.45445\n",
      "step 10, loss: 0.01591, f1 score: 0.50726\n",
      "step 20, loss: 0.01495, f1 score: 0.55330\n",
      "step 30, loss: 0.02037, f1 score: 0.59015\n",
      "step 40, loss: 0.01680, f1 score: 0.62157\n",
      "-- Step: 40\n",
      "eval loss: 0.45162, f1 score: 0.44208\n",
      "step 50, loss: 0.02575, f1 score: 0.64904\n",
      "step 60, loss: 0.01282, f1 score: 0.67117\n",
      "step 70, loss: 0.01347, f1 score: 0.69304\n",
      "step 80, loss: 0.01425, f1 score: 0.71061\n",
      "-- Step: 80\n",
      "eval loss: 0.45437, f1 score: 0.44425\n",
      "step 90, loss: 0.01418, f1 score: 0.72597\n",
      "step 100, loss: 0.01184, f1 score: 0.74012\n",
      "step 110, loss: 0.02591, f1 score: 0.75099\n",
      "step 120, loss: 0.01021, f1 score: 0.76169\n",
      "-- Step: 120\n",
      "eval loss: 0.45738, f1 score: 0.45415\n",
      "step 130, loss: 0.01217, f1 score: 0.77208\n",
      "step 140, loss: 0.01854, f1 score: 0.78237\n",
      "step 150, loss: 0.01391, f1 score: 0.79127\n",
      "step 160, loss: 0.01173, f1 score: 0.79814\n",
      "-- Step: 160\n",
      "eval loss: 0.45895, f1 score: 0.45855\n",
      "step 170, loss: 0.01264, f1 score: 0.80637\n",
      "step 180, loss: 0.02285, f1 score: 0.81310\n",
      "step 190, loss: 0.01713, f1 score: 0.81890\n",
      "step 200, loss: 0.03142, f1 score: 0.82430\n",
      "-- Step: 200\n",
      "eval loss: 0.45821, f1 score: 0.46102\n",
      "step 210, loss: 0.01496, f1 score: 0.82991\n",
      "step 220, loss: 0.02457, f1 score: 0.83486\n",
      "step 230, loss: 0.01439, f1 score: 0.83931\n",
      "step 240, loss: 0.02761, f1 score: 0.84405\n",
      "-- Step: 240\n",
      "eval loss: 0.45934, f1 score: 0.46026\n",
      "step 250, loss: 0.04300, f1 score: 0.84825\n",
      "step 260, loss: 0.02071, f1 score: 0.85197\n",
      "step 270, loss: 0.01408, f1 score: 0.85572\n",
      "step 280, loss: 0.01512, f1 score: 0.85928\n",
      "-- Step: 280\n",
      "eval loss: 0.45698, f1 score: 0.45779\n",
      "step 290, loss: 0.01304, f1 score: 0.86227\n",
      "step 300, loss: 0.01025, f1 score: 0.86563\n",
      "step 310, loss: 0.01295, f1 score: 0.86863\n",
      "step 320, loss: 0.01113, f1 score: 0.87169\n",
      "-- Step: 320\n",
      "eval loss: 0.45732, f1 score: 0.45843\n",
      "step 330, loss: 0.01130, f1 score: 0.87457\n",
      "eval loss: 0.45841, f1 score: 0.45658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f012b949f4ef5a9d0ef84942c4f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.01024, f1 score: 0.46607\n",
      "-- Step: 0\n",
      "eval loss: 0.45843, f1 score: 0.45826\n",
      "step 10, loss: 0.00996, f1 score: 0.52609\n",
      "step 20, loss: 0.00810, f1 score: 0.56518\n",
      "step 30, loss: 0.04303, f1 score: 0.60028\n",
      "step 40, loss: 0.02528, f1 score: 0.63105\n",
      "-- Step: 40\n",
      "eval loss: 0.46454, f1 score: 0.45416\n",
      "step 50, loss: 0.01019, f1 score: 0.65698\n",
      "step 60, loss: 0.00847, f1 score: 0.68018\n",
      "step 70, loss: 0.01085, f1 score: 0.69690\n",
      "step 80, loss: 0.00889, f1 score: 0.71412\n",
      "-- Step: 80\n",
      "eval loss: 0.46924, f1 score: 0.45187\n",
      "step 90, loss: 0.01350, f1 score: 0.73083\n",
      "step 100, loss: 0.00754, f1 score: 0.74324\n",
      "step 110, loss: 0.01930, f1 score: 0.75530\n",
      "step 120, loss: 0.02478, f1 score: 0.76536\n",
      "-- Step: 120\n",
      "eval loss: 0.47454, f1 score: 0.45369\n",
      "step 130, loss: 0.00757, f1 score: 0.77407\n",
      "step 140, loss: 0.01144, f1 score: 0.78323\n",
      "step 150, loss: 0.01289, f1 score: 0.79129\n",
      "step 160, loss: 0.00606, f1 score: 0.79913\n",
      "-- Step: 160\n",
      "eval loss: 0.47361, f1 score: 0.45918\n",
      "step 170, loss: 0.02186, f1 score: 0.80687\n",
      "step 180, loss: 0.00735, f1 score: 0.81321\n",
      "step 190, loss: 0.02703, f1 score: 0.81917\n",
      "step 200, loss: 0.01026, f1 score: 0.82497\n",
      "-- Step: 200\n",
      "eval loss: 0.47464, f1 score: 0.44780\n",
      "step 210, loss: 0.00670, f1 score: 0.83066\n",
      "step 220, loss: 0.00754, f1 score: 0.83599\n",
      "step 230, loss: 0.00868, f1 score: 0.84094\n",
      "step 240, loss: 0.00805, f1 score: 0.84675\n",
      "-- Step: 240\n",
      "eval loss: 0.47670, f1 score: 0.45453\n",
      "step 250, loss: 0.00801, f1 score: 0.85101\n",
      "step 260, loss: 0.00704, f1 score: 0.85494\n",
      "step 270, loss: 0.00785, f1 score: 0.85857\n",
      "step 280, loss: 0.01627, f1 score: 0.86159\n",
      "-- Step: 280\n",
      "eval loss: 0.47497, f1 score: 0.45493\n",
      "step 290, loss: 0.01886, f1 score: 0.86496\n",
      "step 300, loss: 0.00962, f1 score: 0.86774\n",
      "step 310, loss: 0.00834, f1 score: 0.87079\n",
      "step 320, loss: 0.00532, f1 score: 0.87359\n",
      "-- Step: 320\n",
      "eval loss: 0.47652, f1 score: 0.45777\n",
      "step 330, loss: 0.02145, f1 score: 0.87636\n",
      "eval loss: 0.47804, f1 score: 0.46186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cbdf6fbab245ff9deaeb1c75355089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00663, f1 score: 0.46626\n",
      "-- Step: 0\n",
      "eval loss: 0.47817, f1 score: 0.46162\n",
      "step 10, loss: 0.00694, f1 score: 0.51832\n",
      "step 20, loss: 0.01774, f1 score: 0.56170\n",
      "step 30, loss: 0.00644, f1 score: 0.59900\n",
      "step 40, loss: 0.00726, f1 score: 0.62980\n",
      "-- Step: 40\n",
      "eval loss: 0.47945, f1 score: 0.45578\n",
      "step 50, loss: 0.01847, f1 score: 0.65686\n",
      "step 60, loss: 0.00809, f1 score: 0.68070\n",
      "step 70, loss: 0.01313, f1 score: 0.70053\n",
      "step 80, loss: 0.00529, f1 score: 0.71791\n",
      "-- Step: 80\n",
      "eval loss: 0.48363, f1 score: 0.45385\n",
      "step 90, loss: 0.00768, f1 score: 0.73423\n",
      "step 100, loss: 0.00634, f1 score: 0.74697\n",
      "step 110, loss: 0.01702, f1 score: 0.75755\n",
      "step 120, loss: 0.00780, f1 score: 0.76845\n",
      "-- Step: 120\n",
      "eval loss: 0.48157, f1 score: 0.45741\n",
      "step 130, loss: 0.01115, f1 score: 0.77911\n",
      "step 140, loss: 0.00638, f1 score: 0.78828\n",
      "step 150, loss: 0.00551, f1 score: 0.79567\n",
      "step 160, loss: 0.00758, f1 score: 0.80352\n",
      "-- Step: 160\n",
      "eval loss: 0.48860, f1 score: 0.45302\n",
      "step 170, loss: 0.00587, f1 score: 0.81050\n",
      "step 180, loss: 0.02006, f1 score: 0.81777\n",
      "step 190, loss: 0.02603, f1 score: 0.82389\n",
      "step 200, loss: 0.00938, f1 score: 0.82884\n",
      "-- Step: 200\n",
      "eval loss: 0.48651, f1 score: 0.45140\n",
      "step 210, loss: 0.00814, f1 score: 0.83420\n",
      "step 220, loss: 0.00645, f1 score: 0.83874\n",
      "step 230, loss: 0.00540, f1 score: 0.84308\n",
      "step 240, loss: 0.00562, f1 score: 0.84741\n",
      "-- Step: 240\n",
      "eval loss: 0.48576, f1 score: 0.45144\n",
      "step 250, loss: 0.00495, f1 score: 0.85160\n",
      "step 260, loss: 0.00539, f1 score: 0.85528\n",
      "step 270, loss: 0.00948, f1 score: 0.85841\n",
      "step 280, loss: 0.02325, f1 score: 0.86218\n",
      "-- Step: 280\n",
      "eval loss: 0.48936, f1 score: 0.44967\n",
      "step 290, loss: 0.00592, f1 score: 0.86565\n",
      "step 300, loss: 0.01318, f1 score: 0.86850\n",
      "step 310, loss: 0.00546, f1 score: 0.87140\n",
      "step 320, loss: 0.03123, f1 score: 0.87420\n",
      "-- Step: 320\n",
      "eval loss: 0.48714, f1 score: 0.45182\n",
      "step 330, loss: 0.00582, f1 score: 0.87691\n",
      "eval loss: 0.48957, f1 score: 0.45121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fee8d3b7634f2481b0df05fb4e6fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00514, f1 score: 0.45619\n",
      "-- Step: 0\n",
      "eval loss: 0.48959, f1 score: 0.45154\n",
      "step 10, loss: 0.01749, f1 score: 0.52220\n",
      "step 20, loss: 0.00748, f1 score: 0.57159\n",
      "step 30, loss: 0.00379, f1 score: 0.60702\n",
      "step 40, loss: 0.02355, f1 score: 0.63767\n",
      "-- Step: 40\n",
      "eval loss: 0.49259, f1 score: 0.45277\n",
      "step 50, loss: 0.00671, f1 score: 0.66361\n",
      "step 60, loss: 0.00458, f1 score: 0.68484\n",
      "step 70, loss: 0.01234, f1 score: 0.70376\n",
      "step 80, loss: 0.00673, f1 score: 0.72088\n",
      "-- Step: 80\n",
      "eval loss: 0.49252, f1 score: 0.45639\n",
      "step 90, loss: 0.00468, f1 score: 0.73388\n",
      "step 100, loss: 0.01490, f1 score: 0.74773\n",
      "step 110, loss: 0.01449, f1 score: 0.75986\n",
      "step 120, loss: 0.00516, f1 score: 0.77049\n",
      "-- Step: 120\n",
      "eval loss: 0.49849, f1 score: 0.45193\n",
      "step 130, loss: 0.00483, f1 score: 0.77914\n",
      "step 140, loss: 0.02110, f1 score: 0.78696\n",
      "step 150, loss: 0.00612, f1 score: 0.79553\n",
      "step 160, loss: 0.05010, f1 score: 0.80359\n",
      "-- Step: 160\n",
      "eval loss: 0.49886, f1 score: 0.44936\n",
      "step 170, loss: 0.00410, f1 score: 0.81035\n",
      "step 180, loss: 0.00768, f1 score: 0.81801\n",
      "step 190, loss: 0.00575, f1 score: 0.82336\n",
      "step 200, loss: 0.02214, f1 score: 0.82903\n",
      "-- Step: 200\n",
      "eval loss: 0.49905, f1 score: 0.45312\n",
      "step 210, loss: 0.00440, f1 score: 0.83446\n",
      "step 220, loss: 0.00418, f1 score: 0.83987\n",
      "step 230, loss: 0.03107, f1 score: 0.84428\n",
      "step 240, loss: 0.01559, f1 score: 0.84829\n",
      "-- Step: 240\n",
      "eval loss: 0.50206, f1 score: 0.44529\n",
      "step 250, loss: 0.00436, f1 score: 0.85251\n",
      "step 260, loss: 0.01381, f1 score: 0.85624\n",
      "step 270, loss: 0.00631, f1 score: 0.85988\n",
      "step 280, loss: 0.03774, f1 score: 0.86286\n",
      "-- Step: 280\n",
      "eval loss: 0.50098, f1 score: 0.44871\n",
      "step 290, loss: 0.02729, f1 score: 0.86551\n",
      "step 300, loss: 0.00622, f1 score: 0.86860\n",
      "step 310, loss: 0.01880, f1 score: 0.87186\n",
      "step 320, loss: 0.00374, f1 score: 0.87441\n",
      "-- Step: 320\n",
      "eval loss: 0.49744, f1 score: 0.45415\n",
      "step 330, loss: 0.00395, f1 score: 0.87719\n",
      "eval loss: 0.50010, f1 score: 0.44835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9f0b3e622046d791347a846b459e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00633, f1 score: 0.45715\n",
      "-- Step: 0\n",
      "eval loss: 0.50021, f1 score: 0.44853\n",
      "step 10, loss: 0.00457, f1 score: 0.51488\n",
      "step 20, loss: 0.00385, f1 score: 0.55930\n",
      "step 30, loss: 0.00498, f1 score: 0.59161\n",
      "step 40, loss: 0.00769, f1 score: 0.61812\n",
      "-- Step: 40\n",
      "eval loss: 0.50081, f1 score: 0.45317\n",
      "step 50, loss: 0.00516, f1 score: 0.64847\n",
      "step 60, loss: 0.02464, f1 score: 0.67139\n",
      "step 70, loss: 0.00482, f1 score: 0.69325\n",
      "step 80, loss: 0.00410, f1 score: 0.71120\n",
      "-- Step: 80\n",
      "eval loss: 0.50204, f1 score: 0.45389\n",
      "step 90, loss: 0.00464, f1 score: 0.72639\n",
      "step 100, loss: 0.00395, f1 score: 0.74055\n",
      "step 110, loss: 0.02053, f1 score: 0.75522\n",
      "step 120, loss: 0.04708, f1 score: 0.76504\n",
      "-- Step: 120\n",
      "eval loss: 0.50552, f1 score: 0.44672\n",
      "step 130, loss: 0.00401, f1 score: 0.77523\n",
      "step 140, loss: 0.00493, f1 score: 0.78525\n",
      "step 150, loss: 0.00490, f1 score: 0.79404\n",
      "step 160, loss: 0.00474, f1 score: 0.80187\n",
      "-- Step: 160\n",
      "eval loss: 0.50568, f1 score: 0.45571\n",
      "step 170, loss: 0.00528, f1 score: 0.81025\n",
      "step 180, loss: 0.00807, f1 score: 0.81672\n",
      "step 190, loss: 0.00713, f1 score: 0.82402\n",
      "step 200, loss: 0.00424, f1 score: 0.83036\n",
      "-- Step: 200\n",
      "eval loss: 0.50720, f1 score: 0.45227\n",
      "step 210, loss: 0.00735, f1 score: 0.83536\n",
      "step 220, loss: 0.00393, f1 score: 0.84087\n",
      "step 230, loss: 0.00426, f1 score: 0.84517\n",
      "step 240, loss: 0.01713, f1 score: 0.84908\n",
      "-- Step: 240\n",
      "eval loss: 0.50845, f1 score: 0.44857\n",
      "step 250, loss: 0.00442, f1 score: 0.85337\n",
      "step 260, loss: 0.02629, f1 score: 0.85720\n",
      "step 270, loss: 0.00434, f1 score: 0.86020\n",
      "step 280, loss: 0.01504, f1 score: 0.86363\n",
      "-- Step: 280\n",
      "eval loss: 0.50590, f1 score: 0.44990\n",
      "step 290, loss: 0.00443, f1 score: 0.86659\n",
      "step 300, loss: 0.01407, f1 score: 0.86966\n",
      "step 310, loss: 0.00386, f1 score: 0.87243\n",
      "step 320, loss: 0.00412, f1 score: 0.87510\n",
      "-- Step: 320\n",
      "eval loss: 0.50539, f1 score: 0.45377\n",
      "step 330, loss: 0.03210, f1 score: 0.87719\n",
      "eval loss: 0.50586, f1 score: 0.45270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faaf5527a7f4e1bbe56e3ce5728ccec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00430, f1 score: 0.45896\n",
      "-- Step: 0\n",
      "eval loss: 0.50600, f1 score: 0.45273\n",
      "step 10, loss: 0.01343, f1 score: 0.51384\n",
      "step 20, loss: 0.00402, f1 score: 0.56518\n",
      "step 30, loss: 0.01598, f1 score: 0.60877\n",
      "step 40, loss: 0.00437, f1 score: 0.63893\n",
      "-- Step: 40\n",
      "eval loss: 0.50801, f1 score: 0.45432\n",
      "step 50, loss: 0.00363, f1 score: 0.66278\n",
      "step 60, loss: 0.01060, f1 score: 0.68471\n",
      "step 70, loss: 0.00339, f1 score: 0.70286\n",
      "step 80, loss: 0.02923, f1 score: 0.72045\n",
      "-- Step: 80\n",
      "eval loss: 0.51103, f1 score: 0.44938\n",
      "step 90, loss: 0.00472, f1 score: 0.73584\n",
      "step 100, loss: 0.00334, f1 score: 0.74901\n",
      "step 110, loss: 0.00444, f1 score: 0.76063\n",
      "step 120, loss: 0.00423, f1 score: 0.77122\n",
      "-- Step: 120\n",
      "eval loss: 0.51380, f1 score: 0.45143\n",
      "step 130, loss: 0.00458, f1 score: 0.78151\n",
      "step 140, loss: 0.00327, f1 score: 0.79141\n",
      "step 150, loss: 0.00591, f1 score: 0.79936\n",
      "step 160, loss: 0.01181, f1 score: 0.80657\n",
      "-- Step: 160\n",
      "eval loss: 0.51400, f1 score: 0.44963\n",
      "step 170, loss: 0.02525, f1 score: 0.81308\n",
      "step 180, loss: 0.00779, f1 score: 0.81911\n",
      "step 190, loss: 0.00302, f1 score: 0.82430\n",
      "step 200, loss: 0.00295, f1 score: 0.83049\n",
      "-- Step: 200\n",
      "eval loss: 0.51577, f1 score: 0.44831\n",
      "step 210, loss: 0.00396, f1 score: 0.83585\n",
      "step 220, loss: 0.01517, f1 score: 0.84043\n",
      "step 230, loss: 0.02542, f1 score: 0.84423\n",
      "step 240, loss: 0.00394, f1 score: 0.84843\n",
      "-- Step: 240\n",
      "eval loss: 0.51373, f1 score: 0.45387\n",
      "step 250, loss: 0.00403, f1 score: 0.85269\n",
      "step 260, loss: 0.00451, f1 score: 0.85676\n",
      "step 270, loss: 0.02137, f1 score: 0.86002\n",
      "step 280, loss: 0.00422, f1 score: 0.86322\n",
      "-- Step: 280\n",
      "eval loss: 0.51551, f1 score: 0.44960\n",
      "step 290, loss: 0.00326, f1 score: 0.86616\n",
      "step 300, loss: 0.01130, f1 score: 0.86886\n",
      "step 310, loss: 0.01210, f1 score: 0.87186\n",
      "step 320, loss: 0.00371, f1 score: 0.87451\n",
      "-- Step: 320\n",
      "eval loss: 0.51210, f1 score: 0.45199\n",
      "step 330, loss: 0.00358, f1 score: 0.87726\n",
      "eval loss: 0.51427, f1 score: 0.45027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090fcb36feb7412c9659f8992f65eaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00335, f1 score: 0.45582\n",
      "-- Step: 0\n",
      "eval loss: 0.51422, f1 score: 0.44935\n",
      "step 10, loss: 0.00465, f1 score: 0.51202\n",
      "step 20, loss: 0.00255, f1 score: 0.55619\n",
      "step 30, loss: 0.01149, f1 score: 0.59103\n",
      "step 40, loss: 0.00383, f1 score: 0.62576\n",
      "-- Step: 40\n",
      "eval loss: 0.51559, f1 score: 0.45256\n",
      "step 50, loss: 0.00375, f1 score: 0.64945\n",
      "step 60, loss: 0.00315, f1 score: 0.67345\n",
      "step 70, loss: 0.00348, f1 score: 0.69350\n",
      "step 80, loss: 0.00336, f1 score: 0.71256\n",
      "-- Step: 80\n",
      "eval loss: 0.51857, f1 score: 0.44788\n",
      "step 90, loss: 0.00263, f1 score: 0.72645\n",
      "step 100, loss: 0.00388, f1 score: 0.74103\n",
      "step 110, loss: 0.00370, f1 score: 0.75506\n",
      "step 120, loss: 0.00316, f1 score: 0.76635\n",
      "-- Step: 120\n",
      "eval loss: 0.51728, f1 score: 0.45272\n",
      "step 130, loss: 0.00330, f1 score: 0.77757\n",
      "step 140, loss: 0.01207, f1 score: 0.78603\n",
      "step 150, loss: 0.00323, f1 score: 0.79426\n",
      "step 160, loss: 0.00393, f1 score: 0.80245\n",
      "-- Step: 160\n",
      "eval loss: 0.51893, f1 score: 0.45086\n",
      "step 170, loss: 0.01649, f1 score: 0.81013\n",
      "step 180, loss: 0.01627, f1 score: 0.81645\n",
      "step 190, loss: 0.00853, f1 score: 0.82316\n",
      "step 200, loss: 0.02032, f1 score: 0.82946\n",
      "-- Step: 200\n",
      "eval loss: 0.51803, f1 score: 0.45290\n",
      "step 210, loss: 0.00351, f1 score: 0.83450\n",
      "step 220, loss: 0.00288, f1 score: 0.83958\n",
      "step 230, loss: 0.00367, f1 score: 0.84403\n",
      "step 240, loss: 0.01025, f1 score: 0.84836\n",
      "-- Step: 240\n",
      "eval loss: 0.51903, f1 score: 0.45100\n",
      "step 250, loss: 0.03126, f1 score: 0.85257\n",
      "step 260, loss: 0.00331, f1 score: 0.85628\n",
      "step 270, loss: 0.00263, f1 score: 0.85981\n",
      "step 280, loss: 0.00332, f1 score: 0.86359\n",
      "-- Step: 280\n",
      "eval loss: 0.51964, f1 score: 0.45213\n",
      "step 290, loss: 0.00344, f1 score: 0.86670\n",
      "step 300, loss: 0.00427, f1 score: 0.87009\n",
      "step 310, loss: 0.01369, f1 score: 0.87283\n",
      "step 320, loss: 0.01906, f1 score: 0.87549\n",
      "-- Step: 320\n",
      "eval loss: 0.51909, f1 score: 0.44821\n",
      "step 330, loss: 0.02404, f1 score: 0.87815\n",
      "eval loss: 0.51932, f1 score: 0.44802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d69a522b314443992a5e005b747aa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.00289, f1 score: 0.45382\n",
      "-- Step: 0\n",
      "eval loss: 0.51939, f1 score: 0.44806\n",
      "step 10, loss: 0.01726, f1 score: 0.51464\n",
      "step 20, loss: 0.01572, f1 score: 0.56026\n",
      "step 30, loss: 0.01732, f1 score: 0.60136\n",
      "step 40, loss: 0.00336, f1 score: 0.63118\n",
      "-- Step: 40\n",
      "eval loss: 0.52014, f1 score: 0.45002\n",
      "step 50, loss: 0.00319, f1 score: 0.65590\n",
      "step 60, loss: 0.00316, f1 score: 0.68075\n",
      "step 70, loss: 0.00366, f1 score: 0.69999\n",
      "step 80, loss: 0.00359, f1 score: 0.71679\n",
      "-- Step: 80\n",
      "eval loss: 0.52146, f1 score: 0.44938\n",
      "step 90, loss: 0.00324, f1 score: 0.73181\n",
      "step 100, loss: 0.00385, f1 score: 0.74469\n",
      "step 110, loss: 0.00366, f1 score: 0.75706\n",
      "step 120, loss: 0.01387, f1 score: 0.76720\n",
      "-- Step: 120\n",
      "eval loss: 0.52111, f1 score: 0.44843\n",
      "step 130, loss: 0.00416, f1 score: 0.77674\n",
      "step 140, loss: 0.00352, f1 score: 0.78603\n",
      "step 150, loss: 0.00337, f1 score: 0.79410\n",
      "step 160, loss: 0.00390, f1 score: 0.80213\n",
      "-- Step: 160\n",
      "eval loss: 0.52108, f1 score: 0.45057\n",
      "step 170, loss: 0.01142, f1 score: 0.80950\n",
      "step 180, loss: 0.00340, f1 score: 0.81688\n",
      "step 190, loss: 0.00305, f1 score: 0.82267\n",
      "step 200, loss: 0.00282, f1 score: 0.82876\n",
      "-- Step: 200\n",
      "eval loss: 0.52105, f1 score: 0.45036\n",
      "step 210, loss: 0.00372, f1 score: 0.83430\n",
      "step 220, loss: 0.00362, f1 score: 0.83962\n",
      "step 230, loss: 0.00241, f1 score: 0.84388\n",
      "step 240, loss: 0.00349, f1 score: 0.84830\n",
      "-- Step: 240\n",
      "eval loss: 0.52093, f1 score: 0.45056\n",
      "step 250, loss: 0.01597, f1 score: 0.85282\n",
      "step 260, loss: 0.00344, f1 score: 0.85708\n",
      "step 270, loss: 0.01815, f1 score: 0.86061\n",
      "step 280, loss: 0.00281, f1 score: 0.86400\n",
      "-- Step: 280\n",
      "eval loss: 0.52071, f1 score: 0.45140\n",
      "step 290, loss: 0.00246, f1 score: 0.86697\n",
      "step 300, loss: 0.01737, f1 score: 0.86958\n",
      "step 310, loss: 0.01598, f1 score: 0.87275\n",
      "step 320, loss: 0.00627, f1 score: 0.87563\n",
      "-- Step: 320\n",
      "eval loss: 0.52100, f1 score: 0.45148\n",
      "step 330, loss: 0.00532, f1 score: 0.87826\n"
     ]
    }
   ],
   "source": [
    "best_model, best_F1score = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:02:36.749711Z",
     "iopub.status.busy": "2023-01-13T14:02:36.749259Z",
     "iopub.status.idle": "2023-01-13T14:02:36.772644Z",
     "shell.execute_reply": "2023-01-13T14:02:36.771821Z",
     "shell.execute_reply.started": "2023-01-13T14:02:36.749672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (high_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (attention): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       "  (cls): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:02:50.352555Z",
     "iopub.status.busy": "2023-01-13T14:02:50.351448Z",
     "iopub.status.idle": "2023-01-13T14:02:50.359253Z",
     "shell.execute_reply": "2023-01-13T14:02:50.358210Z",
     "shell.execute_reply.started": "2023-01-13T14:02:50.352510Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = TransformerDataset(test_df, set_type='test')\n",
    "test_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)\n",
    "test2_data = TransformerDataset(test2_df, set_type='test')\n",
    "test2_dataloader = DataLoader(test2_data, batch_size=config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:02:55.712339Z",
     "iopub.status.busy": "2023-01-13T14:02:55.711607Z",
     "iopub.status.idle": "2023-01-13T14:02:55.718555Z",
     "shell.execute_reply": "2023-01-13T14:02:55.717474Z",
     "shell.execute_reply.started": "2023-01-13T14:02:55.712298Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model,test_dataloader):\n",
    "    val_loss = 0\n",
    "    test_pred = []\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = logits.cpu().numpy()\n",
    "            test_pred.extend(logits)\n",
    "\n",
    "    test_pred = np.array(test_pred)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:03:01.686874Z",
     "iopub.status.busy": "2023-01-13T14:03:01.686515Z",
     "iopub.status.idle": "2023-01-13T14:03:28.495245Z",
     "shell.execute_reply": "2023-01-13T14:03:28.494282Z",
     "shell.execute_reply.started": "2023-01-13T14:03:01.686843Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = predict(best_model,test_dataloader)\n",
    "test2_pred = predict(best_model,test2_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:03:42.010005Z",
     "iopub.status.busy": "2023-01-13T14:03:42.009619Z",
     "iopub.status.idle": "2023-01-13T14:03:42.015980Z",
     "shell.execute_reply": "2023-01-13T14:03:42.014805Z",
     "shell.execute_reply.started": "2023-01-13T14:03:42.009971Z"
    }
   },
   "outputs": [],
   "source": [
    "test_probs = F.sigmoid(torch.tensor(test_pred))\n",
    "test2_probs = F.sigmoid(torch.tensor(test2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:04:52.533834Z",
     "iopub.status.busy": "2023-01-13T14:04:52.533466Z",
     "iopub.status.idle": "2023-01-13T14:04:52.539187Z",
     "shell.execute_reply": "2023-01-13T14:04:52.538269Z",
     "shell.execute_reply.started": "2023-01-13T14:04:52.533796Z"
    }
   },
   "outputs": [],
   "source": [
    "label_vocab = [\"Self-direction: thought\",\"Self-direction: action\",\"Stimulation\",\"Hedonism\",\"Achievement\",\"Power: dominance\",\"Power: resources\",\"Face\",\"Security: personal\",\"Security: societal\",\"Tradition\",\"Conformity: rules\",\"Conformity: interpersonal\",\"Humility\",\"Benevolence: caring\",\"Benevolence: dependability\",\"Universalism: concern\",\"Universalism: nature\",\"Universalism: tolerance\",\"Universalism: objectivity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:04:54.967952Z",
     "iopub.status.busy": "2023-01-13T14:04:54.967291Z",
     "iopub.status.idle": "2023-01-13T14:04:54.973947Z",
     "shell.execute_reply": "2023-01-13T14:04:54.972707Z",
     "shell.execute_reply.started": "2023-01-13T14:04:54.967917Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeResult(test_probs):\n",
    "    results = []\n",
    "    for prob in test_probs:\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.5:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:04:56.641562Z",
     "iopub.status.busy": "2023-01-13T14:04:56.641194Z",
     "iopub.status.idle": "2023-01-13T14:04:56.877113Z",
     "shell.execute_reply": "2023-01-13T14:04:56.876217Z",
     "shell.execute_reply.started": "2023-01-13T14:04:56.641534Z"
    }
   },
   "outputs": [],
   "source": [
    "results = computeResult(test_probs)\n",
    "results2 = computeResult(test2_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:05:01.492208Z",
     "iopub.status.busy": "2023-01-13T14:05:01.491764Z",
     "iopub.status.idle": "2023-01-13T14:05:01.500688Z",
     "shell.execute_reply": "2023-01-13T14:05:01.499452Z",
     "shell.execute_reply.started": "2023-01-13T14:05:01.492174Z"
    }
   },
   "outputs": [],
   "source": [
    "def testOutput(results2,test_df):\n",
    "    testData = test_df\n",
    "    dicttestT =testData.to_dict(\"list\")\n",
    "\n",
    "    testPred = {}\n",
    "    testPred[\"Argument ID\"] = dicttestT[\"Argument ID\"]\n",
    "    testPred[\"sentence\"] = dicttestT[\"sentence\"]\n",
    "\n",
    "    for x in label_vocab:\n",
    "        testPred[x] = []\n",
    "\n",
    "    for x in range(len(results2)):\n",
    "        types = results2[x].split(\",\")\n",
    "        if types == ['']:\n",
    "            for y in label_vocab:\n",
    "                testPred[y].append(0)\n",
    "        else:  \n",
    "            for z in label_vocab:\n",
    "                if z in types:\n",
    "                    testPred[z].append(1)\n",
    "                else:\n",
    "                    testPred[z].append(0)\n",
    "    testPredD = pd.DataFrame.from_dict(testPred)\n",
    "    testPredD.drop(columns=[\"sentence\"],inplace=True)\n",
    "\n",
    "    return testPredD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:05:07.673990Z",
     "iopub.status.busy": "2023-01-13T14:05:07.673040Z",
     "iopub.status.idle": "2023-01-13T14:05:07.707853Z",
     "shell.execute_reply": "2023-01-13T14:05:07.706966Z",
     "shell.execute_reply.started": "2023-01-13T14:05:07.673954Z"
    }
   },
   "outputs": [],
   "source": [
    "test = testOutput(results,test_df)\n",
    "test.to_csv('/kaggle/working/testRoberta.tsv',columns=test.columns.tolist(),\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:05:09.228392Z",
     "iopub.status.busy": "2023-01-13T14:05:09.226513Z",
     "iopub.status.idle": "2023-01-13T14:05:09.241601Z",
     "shell.execute_reply": "2023-01-13T14:05:09.240656Z",
     "shell.execute_reply.started": "2023-01-13T14:05:09.228342Z"
    }
   },
   "outputs": [],
   "source": [
    "test2 = testOutput(results2,test2_df)\n",
    "test2.to_csv('/kaggle/working/test222Roberta.tsv',columns=test2.columns.tolist(),\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:05:12.798600Z",
     "iopub.status.busy": "2023-01-13T14:05:12.797910Z",
     "iopub.status.idle": "2023-01-13T14:05:40.204136Z",
     "shell.execute_reply": "2023-01-13T14:05:40.203124Z",
     "shell.execute_reply.started": "2023-01-13T14:05:12.798563Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_pred = predict(best_model,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:07:46.776469Z",
     "iopub.status.busy": "2023-01-13T14:07:46.776104Z",
     "iopub.status.idle": "2023-01-13T14:07:47.019371Z",
     "shell.execute_reply": "2023-01-13T14:07:47.018461Z",
     "shell.execute_reply.started": "2023-01-13T14:07:46.776437Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_probs = F.sigmoid(torch.tensor(valid_pred))\n",
    "validresults = computeResult(valid_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:10:38.136141Z",
     "iopub.status.busy": "2023-01-13T14:10:38.135764Z",
     "iopub.status.idle": "2023-01-13T14:10:38.159693Z",
     "shell.execute_reply": "2023-01-13T14:10:38.158861Z",
     "shell.execute_reply.started": "2023-01-13T14:10:38.136105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <th>Self-direction: action</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power: dominance</th>\n",
       "      <th>Power: resources</th>\n",
       "      <th>Face</th>\n",
       "      <th>Security: personal</th>\n",
       "      <th>Security: societal</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Conformity: rules</th>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <th>Humility</th>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <th>Universalism: concern</th>\n",
       "      <th>Universalism: nature</th>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <th>Universalism: objectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Imagine someone is arguing in favor of \"Entrapment should be legalized\" by saying: \"if entrapment can serve to more easily c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01012</td>\n",
       "      <td>Imagine someone is arguing in favor of \"The use of public defenders should be mandatory\" by saying: \"the use of public defen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A02001</td>\n",
       "      <td>Imagine someone is arguing in favor of \"Payday loans should be banned\" by saying: \"payday loans create a more impoverished s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A02002</td>\n",
       "      <td>Imagine someone is arguing against \"Surrogacy should be banned\" by saying: \"Surrogacy should not be banned as it is the woma...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A02009</td>\n",
       "      <td>Imagine someone is arguing against \"Entrapment should be legalized\" by saying: \"entrapment is gravely immoral and against hu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>E08014</td>\n",
       "      <td>Imagine someone is arguing in favor of \"We should shift the EU policy toward the Russian Federation\" by saying: \"Pushing Rus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>E08021</td>\n",
       "      <td>Imagine someone is arguing in favor of \"We should stop buying Russian gas\" by saying: \"The Russians use the money we give th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>E08022</td>\n",
       "      <td>Imagine someone is arguing in favor of \"We should stop buying Russian gas\" by saying: \"The cost of gas will be higher. But I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>E08024</td>\n",
       "      <td>Imagine someone is arguing in favor of \"We should strengthen our ties with Ukraine and prepare for its membership in the EU\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>E08025</td>\n",
       "      <td>Imagine someone is arguing in favor of \"We should strengthen our ties with Ukraine and prepare for its membership in the EU\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1896 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Argument ID  \\\n",
       "0         A01001   \n",
       "1         A01012   \n",
       "2         A02001   \n",
       "3         A02002   \n",
       "4         A02009   \n",
       "...          ...   \n",
       "1891      E08014   \n",
       "1892      E08021   \n",
       "1893      E08022   \n",
       "1894      E08024   \n",
       "1895      E08025   \n",
       "\n",
       "                                                                                                                             sentence  \\\n",
       "0     Imagine someone is arguing in favor of \"Entrapment should be legalized\" by saying: \"if entrapment can serve to more easily c...   \n",
       "1     Imagine someone is arguing in favor of \"The use of public defenders should be mandatory\" by saying: \"the use of public defen...   \n",
       "2     Imagine someone is arguing in favor of \"Payday loans should be banned\" by saying: \"payday loans create a more impoverished s...   \n",
       "3     Imagine someone is arguing against \"Surrogacy should be banned\" by saying: \"Surrogacy should not be banned as it is the woma...   \n",
       "4     Imagine someone is arguing against \"Entrapment should be legalized\" by saying: \"entrapment is gravely immoral and against hu...   \n",
       "...                                                                                                                               ...   \n",
       "1891  Imagine someone is arguing in favor of \"We should shift the EU policy toward the Russian Federation\" by saying: \"Pushing Rus...   \n",
       "1892  Imagine someone is arguing in favor of \"We should stop buying Russian gas\" by saying: \"The Russians use the money we give th...   \n",
       "1893  Imagine someone is arguing in favor of \"We should stop buying Russian gas\" by saying: \"The cost of gas will be higher. But I...   \n",
       "1894  Imagine someone is arguing in favor of \"We should strengthen our ties with Ukraine and prepare for its membership in the EU\"...   \n",
       "1895  Imagine someone is arguing in favor of \"We should strengthen our ties with Ukraine and prepare for its membership in the EU\"...   \n",
       "\n",
       "      Self-direction: thought  Self-direction: action  Stimulation  Hedonism  \\\n",
       "0                           0                       0            0         0   \n",
       "1                           0                       0            0         0   \n",
       "2                           0                       0            0         0   \n",
       "3                           0                       1            0         0   \n",
       "4                           0                       0            0         0   \n",
       "...                       ...                     ...          ...       ...   \n",
       "1891                        1                       0            0         0   \n",
       "1892                        1                       0            0         0   \n",
       "1893                        0                       1            0         0   \n",
       "1894                        0                       1            0         0   \n",
       "1895                        0                       1            0         0   \n",
       "\n",
       "      Achievement  Power: dominance  Power: resources  Face  \\\n",
       "0               0                 0                 0     0   \n",
       "1               0                 0                 0     0   \n",
       "2               0                 0                 0     0   \n",
       "3               0                 0                 0     0   \n",
       "4               0                 0                 0     0   \n",
       "...           ...               ...               ...   ...   \n",
       "1891            1                 0                 0     0   \n",
       "1892            0                 0                 0     0   \n",
       "1893            0                 0                 0     0   \n",
       "1894            0                 1                 0     0   \n",
       "1895            0                 0                 0     0   \n",
       "\n",
       "      Security: personal  Security: societal  Tradition  Conformity: rules  \\\n",
       "0                      0                   1          0                  0   \n",
       "1                      0                   0          0                  0   \n",
       "2                      1                   0          0                  0   \n",
       "3                      0                   0          0                  0   \n",
       "4                      0                   0          0                  1   \n",
       "...                  ...                 ...        ...                ...   \n",
       "1891                   1                   0          0                  1   \n",
       "1892                   0                   1          0                  1   \n",
       "1893                   0                   0          0                  1   \n",
       "1894                   1                   1          0                  0   \n",
       "1895                   0                   0          0                  0   \n",
       "\n",
       "      Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
       "0                             0         0                    0   \n",
       "1                             0         0                    0   \n",
       "2                             0         0                    0   \n",
       "3                             0         0                    0   \n",
       "4                             0         0                    0   \n",
       "...                         ...       ...                  ...   \n",
       "1891                          0         0                    0   \n",
       "1892                          0         0                    0   \n",
       "1893                          0         0                    0   \n",
       "1894                          0         0                    0   \n",
       "1895                          0         0                    0   \n",
       "\n",
       "      Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
       "0                              0                      0                     0   \n",
       "1                              0                      1                     0   \n",
       "2                              0                      1                     0   \n",
       "3                              0                      0                     0   \n",
       "4                              0                      1                     0   \n",
       "...                          ...                    ...                   ...   \n",
       "1891                           0                      1                     0   \n",
       "1892                           1                      1                     0   \n",
       "1893                           1                      1                     0   \n",
       "1894                           0                      1                     0   \n",
       "1895                           0                      1                     0   \n",
       "\n",
       "      Universalism: tolerance  Universalism: objectivity  \n",
       "0                           0                          0  \n",
       "1                           0                          0  \n",
       "2                           0                          0  \n",
       "3                           0                          0  \n",
       "4                           0                          1  \n",
       "...                       ...                        ...  \n",
       "1891                        0                          1  \n",
       "1892                        0                          1  \n",
       "1893                        0                          1  \n",
       "1894                        0                          1  \n",
       "1895                        0                          0  \n",
       "\n",
       "[1896 rows x 22 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:10:46.072227Z",
     "iopub.status.busy": "2023-01-13T14:10:46.071850Z",
     "iopub.status.idle": "2023-01-13T14:10:46.082187Z",
     "shell.execute_reply": "2023-01-13T14:10:46.080973Z",
     "shell.execute_reply.started": "2023-01-13T14:10:46.072195Z"
    }
   },
   "outputs": [],
   "source": [
    "def result2tsv(result1,valid_ds):\n",
    "    validData = valid_ds\n",
    "    dictvalidT =validData.to_dict(\"list\")\n",
    "\n",
    "    validPred = {}\n",
    "    validPred[\"Argument ID\"] = dictvalidT[\"Argument ID\"]\n",
    "    validPred[\"sentence\"] = dictvalidT[\"sentence\"]\n",
    "\n",
    "    for x in label_vocab:\n",
    "        validPred[x] = []\n",
    "    \n",
    "    for x in range(len(result1)):\n",
    "        types = result1[x].split(\",\")\n",
    "        if types == ['']:\n",
    "            for y in label_vocab:\n",
    "                validPred[y].append(0)\n",
    "        else:  \n",
    "            for z in label_vocab:\n",
    "                if z in types:\n",
    "                    validPred[z].append(1)\n",
    "                else:\n",
    "                    validPred[z].append(0)\n",
    "    validData = valid_ds\n",
    "    for x in label_vocab:\n",
    "        for y in range(len(validData[x])):\n",
    "            validData[x].iloc[y] = validPred[x][y]\n",
    "    \n",
    "    validData.drop(columns=[\"sentence\"],inplace=True)\n",
    "    return validData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:10:49.683345Z",
     "iopub.status.busy": "2023-01-13T14:10:49.682628Z",
     "iopub.status.idle": "2023-01-13T14:11:00.566729Z",
     "shell.execute_reply": "2023-01-13T14:11:00.565788Z",
     "shell.execute_reply.started": "2023-01-13T14:10:49.683305Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = result2tsv(validresults,valid_df)\n",
    "valid.to_csv('validRoberta.tsv',columns=valid.columns.tolist(),\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:11:07.945172Z",
     "iopub.status.busy": "2023-01-13T14:11:07.944784Z",
     "iopub.status.idle": "2023-01-13T14:11:07.960493Z",
     "shell.execute_reply": "2023-01-13T14:11:07.959639Z",
     "shell.execute_reply.started": "2023-01-13T14:11:07.945137Z"
    }
   },
   "outputs": [],
   "source": [
    "validzhihu_df = argumentadd(datadir+'/arguments-validation-zhihu.tsv',datadir + '/labels-validation-zhihu.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:11:09.500037Z",
     "iopub.status.busy": "2023-01-13T14:11:09.498954Z",
     "iopub.status.idle": "2023-01-13T14:11:09.505663Z",
     "shell.execute_reply": "2023-01-13T14:11:09.504675Z",
     "shell.execute_reply.started": "2023-01-13T14:11:09.499992Z"
    }
   },
   "outputs": [],
   "source": [
    "validzhihu_data = TransformerDataset(validzhihu_df)\n",
    "\n",
    "validzhihu_dataloader = DataLoader(validzhihu_data, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:12:51.728846Z",
     "iopub.status.busy": "2023-01-13T14:12:51.728484Z",
     "iopub.status.idle": "2023-01-13T14:12:53.233504Z",
     "shell.execute_reply": "2023-01-13T14:12:53.232478Z",
     "shell.execute_reply.started": "2023-01-13T14:12:51.728813Z"
    }
   },
   "outputs": [],
   "source": [
    "validzhihu_pred = predict(best_model,validzhihu_dataloader)\n",
    "validzhihu_probs = F.sigmoid(torch.tensor(validzhihu_pred))\n",
    "validzhihuresults = computeResult(validzhihu_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T14:13:04.140836Z",
     "iopub.status.busy": "2023-01-13T14:13:04.139973Z",
     "iopub.status.idle": "2023-01-13T14:13:04.897390Z",
     "shell.execute_reply": "2023-01-13T14:13:04.896215Z",
     "shell.execute_reply.started": "2023-01-13T14:13:04.140789Z"
    }
   },
   "outputs": [],
   "source": [
    "validzhihu = result2tsv(validzhihuresults,validzhihu_df)\n",
    "validzhihu.to_csv('validzhihuRoberta.tsv',columns=validzhihu.columns.tolist(),\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
