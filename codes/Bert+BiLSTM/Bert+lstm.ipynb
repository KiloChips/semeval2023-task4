{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:42.530561Z",
     "iopub.status.busy": "2023-01-17T10:25:42.529530Z",
     "iopub.status.idle": "2023-01-17T10:25:45.524138Z",
     "shell.execute_reply": "2023-01-17T10:25:45.523186Z",
     "shell.execute_reply.started": "2023-01-17T10:25:42.530515Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.526762Z",
     "iopub.status.busy": "2023-01-17T10:25:45.525867Z",
     "iopub.status.idle": "2023-01-17T10:25:45.534285Z",
     "shell.execute_reply": "2023-01-17T10:25:45.533575Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.526726Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "import re\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "# 清洗无效字符\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# 定义读取数据集函数\n",
    "def read_custom_data(filepath):\n",
    "    f = open(filepath)\n",
    "    next(f)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        labels = [float(d) for d in data[2:]]\n",
    "        yield {\"Argument ID\": data[0], \"sentence\":clean_text(data[1]),\"labels\": labels}\n",
    "    f.close()\n",
    "\n",
    "def read_custom_data_test(filepath):\n",
    "    f = open(filepath)\n",
    "    next(f)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        yield {\"Argument ID\": data[0], \"sentence\":clean_text(data[1]),\"labels\":[]}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:48:50.983583Z",
     "iopub.status.busy": "2023-01-17T11:48:50.982926Z",
     "iopub.status.idle": "2023-01-17T11:48:50.990619Z",
     "shell.execute_reply": "2023-01-17T11:48:50.989627Z",
     "shell.execute_reply.started": "2023-01-17T11:48:50.983540Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data\r\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:49:03.561630Z",
     "iopub.status.busy": "2023-01-17T11:49:03.560794Z",
     "iopub.status.idle": "2023-01-17T11:49:03.814557Z",
     "shell.execute_reply": "2023-01-17T11:49:03.813595Z",
     "shell.execute_reply.started": "2023-01-17T11:49:03.561586Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load_dataset()创建数据集\n",
    "# lazy=False，数据集返回为MapDataset类型\n",
    "# 对训练集和验证集进行预处理\n",
    "train_ds = load_dataset(read_custom_data, filepath='train.tsv', lazy=False) \n",
    "valid_ds = load_dataset(read_custom_data, filepath='validation.tsv', lazy=False)\n",
    "validZhihu_ds = load_dataset(read_custom_data, filepath='zhihu_validation.tsv', lazy=False)\n",
    "test_ds = load_dataset(read_custom_data_test, filepath='test.tsv', lazy=False) \n",
    "test2_ds = load_dataset(read_custom_data_test, filepath='test2.tsv', lazy=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.617515Z",
     "iopub.status.busy": "2023-01-17T10:25:45.617123Z",
     "iopub.status.idle": "2023-01-17T10:25:45.911695Z",
     "shell.execute_reply": "2023-01-17T10:25:45.910560Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.617486Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:25:45,619] [    INFO] - We are using <class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'> to load 'bert-large-uncased'.\r\n",
      "[2023-01-17 18:25:45,622] [    INFO] - Downloading https://bj.bcebos.com/paddle-hapi/models/bert/bert-large-uncased-vocab.txt and saved to /home/aistudio/.paddlenlp/models/bert-large-uncased\r\n",
      "[2023-01-17 18:25:45,625] [    INFO] - Downloading bert-large-uncased-vocab.txt from https://bj.bcebos.com/paddle-hapi/models/bert/bert-large-uncased-vocab.txt\r\n",
      "100%|██████████| 226k/226k [00:00<00:00, 2.39MB/s]\r\n",
      "[2023-01-17 18:25:45,903] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/bert-large-uncased/tokenizer_config.json\r\n",
      "[2023-01-17 18:25:45,906] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/bert-large-uncased/special_tokens_map.json\r\n"
     ]
    }
   ],
   "source": [
    "# 加载中文ERNIE 3.0预训练模型和分词器\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from paddlenlp.transformers import BertTokenizer,BertPretrainedModel\n",
    "from paddlenlp.transformers import BertTokenizer,BertModel\n",
    "\n",
    "model_name = \"bert-large-uncased\"   # ERNIE2.0 模型\n",
    "num_classes = 20  # 20分类任务\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:49:44.512158Z",
     "iopub.status.busy": "2023-01-17T11:49:44.511526Z",
     "iopub.status.idle": "2023-01-17T11:49:44.528556Z",
     "shell.execute_reply": "2023-01-17T11:49:44.527746Z",
     "shell.execute_reply.started": "2023-01-17T11:49:44.512114Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"sentence\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "valid_ds = valid_ds.map(trans_func)\n",
    "validZhihu_ds = validZhihu_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "test2_ds = test2_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "valid_batch_sampler = BatchSampler(valid_ds, batch_size=16, shuffle=False)\n",
    "validZhihu_batch_sampler = BatchSampler(validZhihu_ds, batch_size=16, shuffle=False)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "test2_batch_sampler = BatchSampler(test2_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "valid_data_loader = DataLoader(dataset=valid_ds, batch_sampler=valid_batch_sampler, collate_fn=collate_fn)\n",
    "validZhihu_data_loader = DataLoader(dataset=validZhihu_ds, batch_sampler=validZhihu_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)\n",
    "test2_data_loader = DataLoader(dataset=test2_ds, batch_sampler=test2_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.926991Z",
     "iopub.status.busy": "2023-01-17T10:25:45.926271Z",
     "iopub.status.idle": "2023-01-17T10:25:45.962891Z",
     "shell.execute_reply": "2023-01-17T10:25:45.962026Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.926961Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:25:45,931] [    INFO] - We are using <class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'> to load 'bert-large-uncased'.\r\n",
      "[2023-01-17 18:25:45,933] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-large-uncased/bert-large-uncased-vocab.txt\r\n",
      "[2023-01-17 18:25:45,954] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/bert-large-uncased/tokenizer_config.json\r\n",
      "[2023-01-17 18:25:45,957] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/bert-large-uncased/special_tokens_map.json\r\n"
     ]
    }
   ],
   "source": [
    "#参数设置\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "\n",
    "        self.SEED = 102\n",
    "        self.MODEL_PATH = 'bert-large-uncased'\n",
    "        self.NUM_LABELS = 20\n",
    "\n",
    "        # data\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH = 128\n",
    "        self.BATCH_SIZE = 16\n",
    "\n",
    "        # model\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 3e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        self.N_VALIDATE_DUR_TRAIN = 3\n",
    "        self.N_WARMUP = 0\n",
    "        self.SAVE_BEST_ONLY = True\n",
    "        self.EPOCHS = 20\n",
    "        self.USE_FGM = False\n",
    "        # self.LOSS_TYPE = paddle.nn.BCEWithLogitsLoss()\n",
    "        # self.HIDDEN_DROPOUT_PROB = 0.2\n",
    "        # self.HIDDEN_SIZE = 1024\n",
    "\n",
    "config = Config()\n",
    "import random\n",
    "import numpy as np\n",
    "seed = config.SEED\n",
    "paddle.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.964579Z",
     "iopub.status.busy": "2023-01-17T10:25:45.964023Z",
     "iopub.status.idle": "2023-01-17T10:25:45.967848Z",
     "shell.execute_reply": "2023-01-17T10:25:45.967080Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.964552Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddle.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.969576Z",
     "iopub.status.busy": "2023-01-17T10:25:45.968918Z",
     "iopub.status.idle": "2023-01-17T10:25:45.976369Z",
     "shell.execute_reply": "2023-01-17T10:25:45.975612Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.969549Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bert_LSTM(nn.Layer):\n",
    "  def __init__(self):\n",
    "    super(Bert_LSTM,self).__init__()\n",
    "    self.num_labels=20\n",
    "    self.dropout=nn.Dropout(0.1)\n",
    "    self.bert=BertModel.from_pretrained(config.MODEL_PATH)\n",
    "    for param in self.bert.parameters():\n",
    "      param.requires_grad=True\n",
    "    self.classifier=nn.Linear(1024,self.num_labels)\n",
    "    #self.crf=CRF(num_labels,batch_first=True)\n",
    "\n",
    "    self.bilstm=nn.LSTM(\n",
    "        input_size=1024, \n",
    "        hidden_size=512, \n",
    "        time_major=False,\n",
    "        num_layers=2,\n",
    "        # dropout=0.3,  \n",
    "        direction=\"bidirect\")\n",
    "\n",
    "  def forward(self,input_ids, attention_mask):\n",
    "\n",
    "    output=self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    # pooler_output=output.pooler_output\n",
    "    # last_hidden_state=output.last_hidden_state\n",
    "    last_hidden_state=output[0]\n",
    "\n",
    "    \n",
    "    last_hidden_state=self.dropout(last_hidden_state)\n",
    "    lstm_output,(hn,cn)=self.bilstm(last_hidden_state)\n",
    "    lstm_output=self.dropout(lstm_output)\n",
    "    out =lstm_output[:,-1,:]   #只要序列中最后一个token对应的输出，（因为lstm会记录前边token的信息）\n",
    "\n",
    "    # 得到判别值\n",
    "    logits=self.classifier(out)\n",
    "    # logits=self.classifier(last_hidden_state)\n",
    "    #log_probs = F.log_softmax(logits,dim=-1)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:25:45.980150Z",
     "iopub.status.busy": "2023-01-17T10:25:45.979657Z",
     "iopub.status.idle": "2023-01-17T10:26:57.971760Z",
     "shell.execute_reply": "2023-01-17T10:26:57.970503Z",
     "shell.execute_reply.started": "2023-01-17T10:25:45.980124Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:25:45,981] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/bert-large-uncased.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-large-uncased\r\n",
      "[2023-01-17 18:25:45,984] [    INFO] - Downloading bert-large-uncased.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/bert-large-uncased.pdparams\r\n",
      "100%|██████████| 2.06G/2.06G [00:56<00:00, 39.2MB/s]\r\n",
      "W0117 18:26:42.513813   534 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0117 18:26:42.518093   534 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "[2023-01-17 18:26:57,025] [    INFO] - Weights from pretrained model not used in BertModel: ['cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.weight', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\r\n"
     ]
    }
   ],
   "source": [
    "model = Bert_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:57.974419Z",
     "iopub.status.busy": "2023-01-17T10:26:57.973294Z",
     "iopub.status.idle": "2023-01-17T10:26:57.988631Z",
     "shell.execute_reply": "2023-01-17T10:26:57.987846Z",
     "shell.execute_reply.started": "2023-01-17T10:26:57.974384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert_LSTM(\n",
       "  (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, sparse=False)\n",
       "      (position_embeddings): Embedding(512, 1024, sparse=False)\n",
       "      (token_type_embeddings): Embedding(2, 1024, sparse=False)\n",
       "      (layer_norm): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): LayerList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (12): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (13): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (14): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (15): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (16): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (17): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (18): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (19): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (20): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (21): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (22): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (23): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=20, dtype=float32)\n",
       "  (bilstm): LSTM(1024, 512, num_layers=2\n",
       "    (0): BiRNN(\n",
       "      (cell_fw): LSTMCell(1024, 512)\n",
       "      (cell_bw): LSTMCell(1024, 512)\n",
       "    )\n",
       "    (1): BiRNN(\n",
       "      (cell_fw): LSTMCell(1024, 512)\n",
       "      (cell_bw): LSTMCell(1024, 512)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:57.990543Z",
     "iopub.status.busy": "2023-01-17T10:26:57.989819Z",
     "iopub.status.idle": "2023-01-17T10:26:58.034208Z",
     "shell.execute_reply": "2023-01-17T10:26:58.033198Z",
     "shell.execute_reply.started": "2023-01-17T10:26:57.990516Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# 自定义MultiLabelReport评价指标\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='macro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        thresholds =0\n",
    "        self.y_pred = y_prob > thresholds\n",
    "        score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return f1_score, precison, recall\n",
    "    \n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.036486Z",
     "iopub.status.busy": "2023-01-17T10:26:58.035589Z",
     "iopub.status.idle": "2023-01-17T10:26:58.042610Z",
     "shell.execute_reply": "2023-01-17T10:26:58.041687Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.036451Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def multilabel_categorical_crossentropy(y_pred, y_true):\n",
    "    \"\"\"多标签分类的交叉熵\n",
    "    说明：y_true和y_pred的shape一致，y_true的元素非0即1，\n",
    "         1表示对应的类为目标类，0表示对应的类为非目标类。\n",
    "    警告：请保证y_pred的值域是全体实数，换言之一般情况下y_pred\n",
    "         不用加激活函数，尤其是不能加sigmoid或者softmax！预测\n",
    "         阶段则输出y_pred大于0的类。如有疑问，请仔细阅读并理解\n",
    "         本文。\n",
    "    假如类别总数为10\n",
    "    label ：[0,1,0,0,0,0,0,0,0,1]  代表条数据被标注为 2,10 属于 2类也属于10类\n",
    "    输出也为10类别 输出维度也为10。\n",
    "    类别从1位置开始0位置代表阈值s就是输出的维度第一个位置是阈值预测\n",
    "    目标类的分数都大于s，非目标类的分数都小于s\n",
    "    这里阈值s默认为0故而可忽略只要类从1开始就可\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred\n",
    "    y_pred_neg = y_pred - y_true * 1e12\n",
    "    y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
    "\n",
    "\n",
    "    zeros = paddle.zeros_like(y_pred[..., :1])\n",
    "\n",
    "    y_pred_neg = paddle.concat((y_pred_neg, zeros), axis=-1)\n",
    "    y_pred_pos = paddle.concat((y_pred_pos, zeros), axis=-1)\n",
    "\n",
    "\n",
    "    neg_loss = paddle.logsumexp(y_pred_neg, axis=-1)\n",
    "    pos_loss = paddle.logsumexp(y_pred_pos, axis=-1)\n",
    "    sumloss = (neg_loss + pos_loss).mean()\n",
    "    return sumloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.044511Z",
     "iopub.status.busy": "2023-01-17T10:26:58.043831Z",
     "iopub.status.idle": "2023-01-17T10:26:58.056126Z",
     "shell.execute_reply": "2023-01-17T10:26:58.055351Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.044482Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#定义优化器\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "train_steps_per_epoch=len(train_data_loader)\n",
    "num_training_steps=train_steps_per_epoch*config.EPOCHS\n",
    "\n",
    "#定义各模块参数\n",
    "bert_parameters=list(model.bert.named_parameters())\n",
    "lstm_parameters=list(model.bilstm.named_parameters())\n",
    "classifier_parameters=list(model.classifier.named_parameters())\n",
    "no_decay=['bias','LayerNorm.weight']\n",
    "\n",
    "lr = config.LR\n",
    "\n",
    "#bert模型、lstm模型、nn.linear的学习率分离，后两个是bert的3倍\n",
    "optimizer_grouped_parameters=[\n",
    "    {'params':[p for n,p in bert_parameters if not any(nd in n for nd in no_decay)],\n",
    "      'lr':lr,'weight_decay':0.01},\n",
    "    {'params':[p for n,p in bert_parameters if any(nd in n for nd in no_decay)],\n",
    "      'lr':lr,'weight_decay':0.0},\n",
    "    {'params':[p for n,p in lstm_parameters if not any(nd in n for nd in no_decay)],\n",
    "      'lr':lr*3,'weight_decay':0.01},\n",
    "    {'params':[p for n,p in lstm_parameters if any(nd in n for nd in no_decay)],\n",
    "      'lr':lr*3,'weight_decay': 0.0},\n",
    "    {'params':[p for n,p in classifier_parameters if not any(nd in n for nd in no_decay)],\n",
    "      'lr':lr*3,'weight_decay':0.01},\n",
    "    {'params':[p for n,p in classifier_parameters if any(nd in n for nd in no_decay)],\n",
    "      'lr':lr*3,'weight_decay':0.0}]\n",
    "\n",
    "\n",
    "scheduler = LinearDecayWithWarmup(lr,\n",
    "    num_training_steps,\n",
    "    warmup=0\n",
    ")\n",
    "optimizer = paddle.optimizer.AdamW(scheduler, parameters=optimizer_grouped_parameters, weight_decay=0.01)\n",
    "criterion = multilabel_categorical_crossentropy\n",
    "metric = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.057345Z",
     "iopub.status.busy": "2023-01-17T10:26:58.057114Z",
     "iopub.status.idle": "2023-01-17T10:26:58.065736Z",
     "shell.execute_reply": "2023-01-17T10:26:58.065006Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.057324Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 构建验证集evaluate函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        # probs = F.sigmoid(logits)\n",
    "        probs = logits\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0:\n",
    "                        result.append(label_vocab[c])\n",
    "                        # result.append(str(c))\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    # auc, f1_score, precison, recall = metric.accumulate()\n",
    "    f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.067280Z",
     "iopub.status.busy": "2023-01-17T10:26:58.066793Z",
     "iopub.status.idle": "2023-01-17T10:26:58.070995Z",
     "shell.execute_reply": "2023-01-17T10:26:58.070237Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.067256Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_vocab = [\"Self-direction: thought\",\"Self-direction: action\",\"Stimulation\",\"Hedonism\",\"Achievement\",\"Power: dominance\",\"Power: resources\",\"Face\",\"Security: personal\",\"Security: societal\",\"Tradition\",\"Conformity: rules\",\"Conformity: interpersonal\",\"Humility\",\"Benevolence: caring\",\"Benevolence: dependability\",\"Universalism: concern\",\"Universalism: nature\",\"Universalism: tolerance\",\"Universalism: objectivity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.072472Z",
     "iopub.status.busy": "2023-01-17T10:26:58.072004Z",
     "iopub.status.idle": "2023-01-17T10:26:58.078342Z",
     "shell.execute_reply": "2023-01-17T10:26:58.077383Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.072448Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/model\r\n"
     ]
    }
   ],
   "source": [
    "cd /home/aistudio/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T10:26:58.079835Z",
     "iopub.status.busy": "2023-01-17T10:26:58.079479Z",
     "iopub.status.idle": "2023-01-17T11:35:50.842702Z",
     "shell.execute_reply": "2023-01-17T11:35:50.841815Z",
     "shell.execute_reply.started": "2023-01-17T10:26:58.079805Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 4.17505, f1 score: 0.19221, speed: 1.32 step/s\r\n",
      "global step 20, epoch: 1, batch: 20, loss: 4.00801, f1 score: 0.17947, speed: 1.40 step/s\r\n",
      "global step 30, epoch: 1, batch: 30, loss: 4.08815, f1 score: 0.16218, speed: 1.35 step/s\r\n",
      "global step 40, epoch: 1, batch: 40, loss: 3.98313, f1 score: 0.14874, speed: 1.63 step/s\r\n",
      "eval loss: 4.02354, f1 score: 0.02721, precison: 0.05562, recall: 0.05031\r\n",
      "eval loss: 3.78912, f1 score: 0.01736, precison: 0.01050, recall: 0.05000\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:28:17,443] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:28:17,447] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 50, epoch: 1, batch: 50, loss: 4.19851, f1 score: 0.04540, speed: 0.18 step/s\r\n",
      "global step 60, epoch: 1, batch: 60, loss: 3.91001, f1 score: 0.03824, speed: 1.60 step/s\r\n",
      "global step 70, epoch: 1, batch: 70, loss: 4.08896, f1 score: 0.03381, speed: 1.51 step/s\r\n",
      "global step 80, epoch: 1, batch: 80, loss: 4.05187, f1 score: 0.02902, speed: 1.40 step/s\r\n",
      "eval loss: 4.01776, f1 score: 0.00463, precison: 0.04468, recall: 0.00258\r\n",
      "eval loss: 3.76241, f1 score: 0.00270, precison: 0.00714, recall: 0.00167\r\n",
      "global step 90, epoch: 1, batch: 90, loss: 4.00487, f1 score: 0.01359, speed: 0.52 step/s\r\n",
      "global step 100, epoch: 1, batch: 100, loss: 4.07057, f1 score: 0.01656, speed: 1.48 step/s\r\n",
      "global step 110, epoch: 1, batch: 110, loss: 3.94773, f1 score: 0.01224, speed: 1.50 step/s\r\n",
      "global step 120, epoch: 1, batch: 120, loss: 4.01879, f1 score: 0.02513, speed: 1.60 step/s\r\n",
      "eval loss: 4.00904, f1 score: 0.02680, precison: 0.01852, recall: 0.04847\r\n",
      "eval loss: 3.75796, f1 score: 0.01736, precison: 0.01050, recall: 0.05000\r\n",
      "global step 130, epoch: 1, batch: 130, loss: 3.96650, f1 score: 0.03586, speed: 0.51 step/s\r\n",
      "global step 140, epoch: 1, batch: 140, loss: 3.90233, f1 score: 0.06348, speed: 1.35 step/s\r\n",
      "global step 150, epoch: 1, batch: 150, loss: 3.72537, f1 score: 0.10313, speed: 1.57 step/s\r\n",
      "global step 160, epoch: 1, batch: 160, loss: 3.83451, f1 score: 0.13836, speed: 1.47 step/s\r\n",
      "eval loss: 3.81063, f1 score: 0.18442, precison: 0.32220, recall: 0.16966\r\n",
      "eval loss: 3.55971, f1 score: 0.11536, precison: 0.15740, recall: 0.11866\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:30:53,665] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:30:53,669] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 170, epoch: 2, batch: 1, loss: 3.79554, f1 score: 0.26347, speed: 0.17 step/s\r\n",
      "global step 180, epoch: 2, batch: 11, loss: 3.74101, f1 score: 0.26199, speed: 1.51 step/s\r\n",
      "global step 190, epoch: 2, batch: 21, loss: 3.43290, f1 score: 0.27212, speed: 1.53 step/s\r\n",
      "global step 200, epoch: 2, batch: 31, loss: 3.76487, f1 score: 0.27829, speed: 1.62 step/s\r\n",
      "eval loss: 3.68686, f1 score: 0.27102, precison: 0.37637, recall: 0.24121\r\n",
      "eval loss: 3.43917, f1 score: 0.20729, precison: 0.25095, recall: 0.22117\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:32:10,743] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:32:10,747] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 210, epoch: 2, batch: 41, loss: 3.66456, f1 score: 0.31147, speed: 0.17 step/s\r\n",
      "global step 220, epoch: 2, batch: 51, loss: 3.70721, f1 score: 0.30044, speed: 1.49 step/s\r\n",
      "global step 230, epoch: 2, batch: 61, loss: 3.58504, f1 score: 0.30747, speed: 1.56 step/s\r\n",
      "global step 240, epoch: 2, batch: 71, loss: 3.42028, f1 score: 0.31811, speed: 1.51 step/s\r\n",
      "eval loss: 3.62001, f1 score: 0.28151, precison: 0.49167, recall: 0.24358\r\n",
      "eval loss: 3.46360, f1 score: 0.19137, precison: 0.19433, recall: 0.20766\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:33:28,370] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:33:28,374] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 250, epoch: 2, batch: 81, loss: 3.47542, f1 score: 0.29749, speed: 0.17 step/s\r\n",
      "global step 260, epoch: 2, batch: 91, loss: 3.65936, f1 score: 0.31107, speed: 1.55 step/s\r\n",
      "global step 270, epoch: 2, batch: 101, loss: 3.65753, f1 score: 0.32041, speed: 1.42 step/s\r\n",
      "global step 280, epoch: 2, batch: 111, loss: 3.57864, f1 score: 0.32875, speed: 1.38 step/s\r\n",
      "eval loss: 3.59629, f1 score: 0.28774, precison: 0.50397, recall: 0.24350\r\n",
      "eval loss: 3.40104, f1 score: 0.21154, precison: 0.24304, recall: 0.21705\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:34:47,073] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:34:47,077] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 290, epoch: 2, batch: 121, loss: 3.77228, f1 score: 0.33577, speed: 0.17 step/s\r\n",
      "global step 300, epoch: 2, batch: 131, loss: 3.53001, f1 score: 0.35737, speed: 1.45 step/s\r\n",
      "global step 310, epoch: 2, batch: 141, loss: 3.40752, f1 score: 0.36003, speed: 1.51 step/s\r\n",
      "global step 320, epoch: 2, batch: 151, loss: 3.59357, f1 score: 0.35714, speed: 1.45 step/s\r\n",
      "eval loss: 3.61059, f1 score: 0.27521, precison: 0.57437, recall: 0.22696\r\n",
      "eval loss: 3.37940, f1 score: 0.21188, precison: 0.22992, recall: 0.20996\r\n",
      "global step 330, epoch: 2, batch: 161, loss: 3.83676, f1 score: 0.36249, speed: 0.50 step/s\r\n",
      "global step 340, epoch: 3, batch: 2, loss: 3.29862, f1 score: 0.36086, speed: 1.53 step/s\r\n",
      "global step 350, epoch: 3, batch: 12, loss: 3.25089, f1 score: 0.36824, speed: 1.47 step/s\r\n",
      "global step 360, epoch: 3, batch: 22, loss: 3.31865, f1 score: 0.38343, speed: 1.65 step/s\r\n",
      "eval loss: 3.57599, f1 score: 0.33382, precison: 0.56835, recall: 0.27351\r\n",
      "eval loss: 3.35298, f1 score: 0.21010, precison: 0.21527, recall: 0.21610\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:36:45,557] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:36:45,560] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 370, epoch: 3, batch: 32, loss: 3.08145, f1 score: 0.46692, speed: 0.17 step/s\r\n",
      "global step 380, epoch: 3, batch: 42, loss: 3.63447, f1 score: 0.46465, speed: 1.41 step/s\r\n",
      "global step 390, epoch: 3, batch: 52, loss: 3.26290, f1 score: 0.45703, speed: 1.52 step/s\r\n",
      "global step 400, epoch: 3, batch: 62, loss: 2.97674, f1 score: 0.45061, speed: 1.49 step/s\r\n",
      "eval loss: 3.58062, f1 score: 0.35450, precison: 0.61422, recall: 0.29373\r\n",
      "eval loss: 3.44076, f1 score: 0.24938, precison: 0.27475, recall: 0.25457\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:38:04,163] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:38:04,167] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 410, epoch: 3, batch: 72, loss: 3.25746, f1 score: 0.45605, speed: 0.17 step/s\r\n",
      "global step 420, epoch: 3, batch: 82, loss: 3.23852, f1 score: 0.45543, speed: 1.41 step/s\r\n",
      "global step 430, epoch: 3, batch: 92, loss: 3.42116, f1 score: 0.45745, speed: 1.55 step/s\r\n",
      "global step 440, epoch: 3, batch: 102, loss: 3.33683, f1 score: 0.46554, speed: 1.65 step/s\r\n",
      "eval loss: 3.57411, f1 score: 0.35597, precison: 0.58558, recall: 0.28827\r\n",
      "eval loss: 3.30413, f1 score: 0.26980, precison: 0.29452, recall: 0.26257\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:39:22,808] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:39:22,812] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 450, epoch: 3, batch: 112, loss: 3.19904, f1 score: 0.46744, speed: 0.17 step/s\r\n",
      "global step 460, epoch: 3, batch: 122, loss: 3.63720, f1 score: 0.45721, speed: 1.51 step/s\r\n",
      "global step 470, epoch: 3, batch: 132, loss: 2.98776, f1 score: 0.46516, speed: 1.54 step/s\r\n",
      "global step 480, epoch: 3, batch: 142, loss: 2.94023, f1 score: 0.46162, speed: 1.46 step/s\r\n",
      "eval loss: 3.57407, f1 score: 0.35676, precison: 0.61933, recall: 0.29417\r\n",
      "eval loss: 3.35588, f1 score: 0.25421, precison: 0.29025, recall: 0.25915\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:40:41,152] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:40:41,156] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 490, epoch: 3, batch: 152, loss: 3.27022, f1 score: 0.43475, speed: 0.17 step/s\r\n",
      "global step 500, epoch: 3, batch: 162, loss: 3.06265, f1 score: 0.45471, speed: 1.43 step/s\r\n",
      "global step 510, epoch: 4, batch: 3, loss: 3.17545, f1 score: 0.46781, speed: 1.63 step/s\r\n",
      "global step 520, epoch: 4, batch: 13, loss: 2.98507, f1 score: 0.47847, speed: 1.41 step/s\r\n",
      "eval loss: 3.59649, f1 score: 0.36806, precison: 0.58302, recall: 0.30228\r\n",
      "eval loss: 3.30444, f1 score: 0.28010, precison: 0.28724, recall: 0.29316\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:41:59,437] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:41:59,443] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 530, epoch: 4, batch: 23, loss: 2.83050, f1 score: 0.52673, speed: 0.17 step/s\r\n",
      "global step 540, epoch: 4, batch: 33, loss: 3.01043, f1 score: 0.53154, speed: 1.42 step/s\r\n",
      "global step 550, epoch: 4, batch: 43, loss: 2.96290, f1 score: 0.53355, speed: 1.59 step/s\r\n",
      "global step 560, epoch: 4, batch: 53, loss: 3.06767, f1 score: 0.53786, speed: 1.37 step/s\r\n",
      "eval loss: 3.62657, f1 score: 0.39042, precison: 0.62888, recall: 0.32423\r\n",
      "eval loss: 3.37683, f1 score: 0.29188, precison: 0.31715, recall: 0.29769\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:43:19,067] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:43:19,070] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 570, epoch: 4, batch: 63, loss: 3.19608, f1 score: 0.53957, speed: 0.17 step/s\r\n",
      "global step 580, epoch: 4, batch: 73, loss: 2.80591, f1 score: 0.54612, speed: 1.39 step/s\r\n",
      "global step 590, epoch: 4, batch: 83, loss: 2.83892, f1 score: 0.54602, speed: 1.42 step/s\r\n",
      "global step 600, epoch: 4, batch: 93, loss: 3.25182, f1 score: 0.54583, speed: 1.43 step/s\r\n",
      "eval loss: 3.66902, f1 score: 0.37896, precison: 0.60235, recall: 0.32606\r\n",
      "eval loss: 3.39058, f1 score: 0.26985, precison: 0.28794, recall: 0.27480\r\n",
      "global step 610, epoch: 4, batch: 103, loss: 2.84032, f1 score: 0.51215, speed: 0.50 step/s\r\n",
      "global step 620, epoch: 4, batch: 113, loss: 2.79213, f1 score: 0.54032, speed: 1.38 step/s\r\n",
      "global step 630, epoch: 4, batch: 123, loss: 2.85053, f1 score: 0.56429, speed: 1.49 step/s\r\n",
      "global step 640, epoch: 4, batch: 133, loss: 2.92140, f1 score: 0.56413, speed: 1.52 step/s\r\n",
      "eval loss: 3.64026, f1 score: 0.40388, precison: 0.59021, recall: 0.33830\r\n",
      "eval loss: 3.34101, f1 score: 0.25896, precison: 0.27512, recall: 0.27098\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:45:20,035] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:45:20,039] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 650, epoch: 4, batch: 143, loss: 2.89791, f1 score: 0.51158, speed: 0.17 step/s\r\n",
      "global step 660, epoch: 4, batch: 153, loss: 2.86836, f1 score: 0.54498, speed: 1.52 step/s\r\n",
      "global step 670, epoch: 4, batch: 163, loss: 3.09798, f1 score: 0.54100, speed: 1.46 step/s\r\n",
      "global step 680, epoch: 5, batch: 4, loss: 2.81496, f1 score: 0.55548, speed: 1.44 step/s\r\n",
      "eval loss: 3.70512, f1 score: 0.37400, precison: 0.59037, recall: 0.31290\r\n",
      "eval loss: 3.32490, f1 score: 0.24955, precison: 0.27040, recall: 0.26541\r\n",
      "global step 690, epoch: 5, batch: 14, loss: 3.11175, f1 score: 0.58190, speed: 0.50 step/s\r\n",
      "global step 700, epoch: 5, batch: 24, loss: 2.74364, f1 score: 0.60080, speed: 1.54 step/s\r\n",
      "global step 710, epoch: 5, batch: 34, loss: 2.79234, f1 score: 0.59827, speed: 1.66 step/s\r\n",
      "global step 720, epoch: 5, batch: 44, loss: 2.64291, f1 score: 0.60896, speed: 1.38 step/s\r\n",
      "eval loss: 3.77106, f1 score: 0.41119, precison: 0.60048, recall: 0.34550\r\n",
      "eval loss: 3.38747, f1 score: 0.30586, precison: 0.32516, recall: 0.31656\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:47:17,810] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:47:17,813] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 730, epoch: 5, batch: 54, loss: 3.00727, f1 score: 0.65408, speed: 0.17 step/s\r\n",
      "global step 740, epoch: 5, batch: 64, loss: 2.61787, f1 score: 0.63912, speed: 1.48 step/s\r\n",
      "global step 750, epoch: 5, batch: 74, loss: 2.96572, f1 score: 0.63108, speed: 1.54 step/s\r\n",
      "global step 760, epoch: 5, batch: 84, loss: 2.54783, f1 score: 0.63567, speed: 1.58 step/s\r\n",
      "eval loss: 3.81669, f1 score: 0.41298, precison: 0.58019, recall: 0.35026\r\n",
      "eval loss: 3.44590, f1 score: 0.29916, precison: 0.31489, recall: 0.31912\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:48:35,905] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:48:35,909] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 770, epoch: 5, batch: 94, loss: 2.53716, f1 score: 0.64577, speed: 0.17 step/s\r\n",
      "global step 780, epoch: 5, batch: 104, loss: 2.57693, f1 score: 0.63297, speed: 1.45 step/s\r\n",
      "global step 790, epoch: 5, batch: 114, loss: 2.67806, f1 score: 0.63766, speed: 1.55 step/s\r\n",
      "global step 800, epoch: 5, batch: 124, loss: 2.91243, f1 score: 0.62033, speed: 1.41 step/s\r\n",
      "eval loss: 3.81460, f1 score: 0.41498, precison: 0.55833, recall: 0.35860\r\n",
      "eval loss: 3.43005, f1 score: 0.30787, precison: 0.30487, recall: 0.33101\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:49:54,261] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:49:54,265] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 810, epoch: 5, batch: 134, loss: 2.95571, f1 score: 0.63678, speed: 0.17 step/s\r\n",
      "global step 820, epoch: 5, batch: 144, loss: 2.50355, f1 score: 0.63621, speed: 1.41 step/s\r\n",
      "global step 830, epoch: 5, batch: 154, loss: 2.71817, f1 score: 0.61971, speed: 1.56 step/s\r\n",
      "global step 840, epoch: 5, batch: 164, loss: 2.80428, f1 score: 0.62004, speed: 1.45 step/s\r\n",
      "eval loss: 3.82732, f1 score: 0.42371, precison: 0.57591, recall: 0.36592\r\n",
      "eval loss: 3.41452, f1 score: 0.30523, precison: 0.29775, recall: 0.32444\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:51:12,625] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:51:12,628] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 850, epoch: 6, batch: 5, loss: 2.57413, f1 score: 0.62968, speed: 0.17 step/s\r\n",
      "global step 860, epoch: 6, batch: 15, loss: 2.24026, f1 score: 0.65166, speed: 1.49 step/s\r\n",
      "global step 870, epoch: 6, batch: 25, loss: 2.41678, f1 score: 0.67193, speed: 1.43 step/s\r\n",
      "global step 880, epoch: 6, batch: 35, loss: 2.47935, f1 score: 0.67853, speed: 1.49 step/s\r\n",
      "eval loss: 3.96031, f1 score: 0.41603, precison: 0.57917, recall: 0.35870\r\n",
      "eval loss: 3.51742, f1 score: 0.29993, precison: 0.29332, recall: 0.32821\r\n",
      "global step 890, epoch: 6, batch: 45, loss: 2.18344, f1 score: 0.69638, speed: 0.50 step/s\r\n",
      "global step 900, epoch: 6, batch: 55, loss: 2.52250, f1 score: 0.69756, speed: 1.61 step/s\r\n",
      "global step 910, epoch: 6, batch: 65, loss: 2.54254, f1 score: 0.69870, speed: 1.49 step/s\r\n",
      "global step 920, epoch: 6, batch: 75, loss: 2.58954, f1 score: 0.69404, speed: 1.45 step/s\r\n",
      "eval loss: 4.12248, f1 score: 0.40064, precison: 0.58454, recall: 0.35196\r\n",
      "eval loss: 3.64168, f1 score: 0.31006, precison: 0.31538, recall: 0.33708\r\n",
      "global step 930, epoch: 6, batch: 85, loss: 2.38631, f1 score: 0.70717, speed: 0.51 step/s\r\n",
      "global step 940, epoch: 6, batch: 95, loss: 2.13755, f1 score: 0.68611, speed: 1.63 step/s\r\n",
      "global step 950, epoch: 6, batch: 105, loss: 2.46082, f1 score: 0.68025, speed: 1.48 step/s\r\n",
      "global step 960, epoch: 6, batch: 115, loss: 2.60811, f1 score: 0.67999, speed: 1.39 step/s\r\n",
      "eval loss: 4.11485, f1 score: 0.41672, precison: 0.55502, recall: 0.37191\r\n",
      "eval loss: 3.69371, f1 score: 0.28836, precison: 0.30146, recall: 0.32347\r\n",
      "global step 970, epoch: 6, batch: 125, loss: 2.12420, f1 score: 0.66745, speed: 0.52 step/s\r\n",
      "global step 980, epoch: 6, batch: 135, loss: 2.46646, f1 score: 0.66423, speed: 1.39 step/s\r\n",
      "global step 990, epoch: 6, batch: 145, loss: 2.39516, f1 score: 0.67802, speed: 1.46 step/s\r\n",
      "global step 1000, epoch: 6, batch: 155, loss: 2.26464, f1 score: 0.67400, speed: 1.45 step/s\r\n",
      "eval loss: 4.13599, f1 score: 0.39835, precison: 0.56874, recall: 0.33659\r\n",
      "eval loss: 3.74369, f1 score: 0.26625, precison: 0.26347, recall: 0.29318\r\n",
      "global step 1010, epoch: 6, batch: 165, loss: 2.27912, f1 score: 0.65317, speed: 0.51 step/s\r\n",
      "global step 1020, epoch: 7, batch: 6, loss: 1.86142, f1 score: 0.70368, speed: 1.50 step/s\r\n",
      "global step 1030, epoch: 7, batch: 16, loss: 2.15953, f1 score: 0.71783, speed: 1.48 step/s\r\n",
      "global step 1040, epoch: 7, batch: 26, loss: 2.42904, f1 score: 0.71787, speed: 1.38 step/s\r\n",
      "eval loss: 4.21622, f1 score: 0.42495, precison: 0.58571, recall: 0.36503\r\n",
      "eval loss: 3.76514, f1 score: 0.28803, precison: 0.29190, recall: 0.30661\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:55:11,207] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:55:11,211] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1050, epoch: 7, batch: 36, loss: 2.22550, f1 score: 0.73342, speed: 0.17 step/s\r\n",
      "global step 1060, epoch: 7, batch: 46, loss: 2.20347, f1 score: 0.73228, speed: 1.56 step/s\r\n",
      "global step 1070, epoch: 7, batch: 56, loss: 2.20065, f1 score: 0.73084, speed: 1.38 step/s\r\n",
      "global step 1080, epoch: 7, batch: 66, loss: 1.97543, f1 score: 0.73994, speed: 1.41 step/s\r\n",
      "eval loss: 4.23790, f1 score: 0.41796, precison: 0.57377, recall: 0.36288\r\n",
      "eval loss: 3.80046, f1 score: 0.29729, precison: 0.29317, recall: 0.32788\r\n",
      "global step 1090, epoch: 7, batch: 76, loss: 2.19041, f1 score: 0.69279, speed: 0.51 step/s\r\n",
      "global step 1100, epoch: 7, batch: 86, loss: 2.18563, f1 score: 0.72220, speed: 1.56 step/s\r\n",
      "global step 1110, epoch: 7, batch: 96, loss: 2.36460, f1 score: 0.71495, speed: 1.34 step/s\r\n",
      "global step 1120, epoch: 7, batch: 106, loss: 2.33747, f1 score: 0.71169, speed: 1.44 step/s\r\n",
      "eval loss: 4.31408, f1 score: 0.42144, precison: 0.57216, recall: 0.36443\r\n",
      "eval loss: 3.95118, f1 score: 0.31782, precison: 0.31611, recall: 0.35140\r\n",
      "global step 1130, epoch: 7, batch: 116, loss: 1.96207, f1 score: 0.70912, speed: 0.49 step/s\r\n",
      "global step 1140, epoch: 7, batch: 126, loss: 2.14606, f1 score: 0.71904, speed: 1.50 step/s\r\n",
      "global step 1150, epoch: 7, batch: 136, loss: 1.95686, f1 score: 0.73178, speed: 1.47 step/s\r\n",
      "global step 1160, epoch: 7, batch: 146, loss: 1.97840, f1 score: 0.73590, speed: 1.58 step/s\r\n",
      "eval loss: 4.25841, f1 score: 0.45284, precison: 0.55220, recall: 0.40518\r\n",
      "eval loss: 3.83590, f1 score: 0.30627, precison: 0.28232, recall: 0.35938\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 18:57:50,283] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 18:57:50,287] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1170, epoch: 7, batch: 156, loss: 2.15337, f1 score: 0.73694, speed: 0.17 step/s\r\n",
      "global step 1180, epoch: 7, batch: 166, loss: 2.24050, f1 score: 0.72878, speed: 1.60 step/s\r\n",
      "global step 1190, epoch: 8, batch: 7, loss: 1.94417, f1 score: 0.72999, speed: 1.48 step/s\r\n",
      "global step 1200, epoch: 8, batch: 17, loss: 2.01701, f1 score: 0.74743, speed: 1.47 step/s\r\n",
      "eval loss: 4.39105, f1 score: 0.43146, precison: 0.58210, recall: 0.37579\r\n",
      "eval loss: 3.90370, f1 score: 0.32863, precison: 0.31554, recall: 0.37981\r\n",
      "global step 1210, epoch: 8, batch: 27, loss: 1.81266, f1 score: 0.77360, speed: 0.49 step/s\r\n",
      "global step 1220, epoch: 8, batch: 37, loss: 1.68384, f1 score: 0.78153, speed: 1.45 step/s\r\n",
      "global step 1230, epoch: 8, batch: 47, loss: 1.55278, f1 score: 0.78863, speed: 1.44 step/s\r\n",
      "global step 1240, epoch: 8, batch: 57, loss: 1.90063, f1 score: 0.77823, speed: 1.47 step/s\r\n",
      "eval loss: 4.44567, f1 score: 0.43792, precison: 0.56760, recall: 0.38507\r\n",
      "eval loss: 3.93742, f1 score: 0.31959, precison: 0.31384, recall: 0.36508\r\n",
      "global step 1250, epoch: 8, batch: 67, loss: 2.02232, f1 score: 0.79163, speed: 0.50 step/s\r\n",
      "global step 1260, epoch: 8, batch: 77, loss: 2.06306, f1 score: 0.79332, speed: 1.58 step/s\r\n",
      "global step 1270, epoch: 8, batch: 87, loss: 2.02065, f1 score: 0.79154, speed: 1.47 step/s\r\n",
      "global step 1280, epoch: 8, batch: 97, loss: 2.15568, f1 score: 0.78975, speed: 1.47 step/s\r\n",
      "eval loss: 4.51680, f1 score: 0.43865, precison: 0.55331, recall: 0.39219\r\n",
      "eval loss: 3.98253, f1 score: 0.31594, precison: 0.30617, recall: 0.36205\r\n",
      "global step 1290, epoch: 8, batch: 107, loss: 1.88815, f1 score: 0.80484, speed: 0.49 step/s\r\n",
      "global step 1300, epoch: 8, batch: 117, loss: 1.75574, f1 score: 0.76942, speed: 1.45 step/s\r\n",
      "global step 1310, epoch: 8, batch: 127, loss: 2.05521, f1 score: 0.77296, speed: 1.56 step/s\r\n",
      "global step 1320, epoch: 8, batch: 137, loss: 1.92161, f1 score: 0.77452, speed: 1.53 step/s\r\n",
      "eval loss: 4.60364, f1 score: 0.44060, precison: 0.55011, recall: 0.39302\r\n",
      "eval loss: 4.17278, f1 score: 0.31124, precison: 0.31097, recall: 0.35878\r\n",
      "global step 1330, epoch: 8, batch: 147, loss: 1.81883, f1 score: 0.73642, speed: 0.50 step/s\r\n",
      "global step 1340, epoch: 8, batch: 157, loss: 2.45150, f1 score: 0.76096, speed: 1.45 step/s\r\n",
      "global step 1350, epoch: 8, batch: 167, loss: 1.77734, f1 score: 0.77117, speed: 1.40 step/s\r\n",
      "global step 1360, epoch: 9, batch: 8, loss: 1.87370, f1 score: 0.78313, speed: 1.43 step/s\r\n",
      "eval loss: 4.76074, f1 score: 0.41535, precison: 0.58480, recall: 0.35760\r\n",
      "eval loss: 4.10199, f1 score: 0.31645, precison: 0.31715, recall: 0.34582\r\n",
      "global step 1370, epoch: 9, batch: 18, loss: 1.86235, f1 score: 0.80724, speed: 0.49 step/s\r\n",
      "global step 1380, epoch: 9, batch: 28, loss: 1.56325, f1 score: 0.82729, speed: 1.58 step/s\r\n",
      "global step 1390, epoch: 9, batch: 38, loss: 1.61710, f1 score: 0.81941, speed: 1.54 step/s\r\n",
      "global step 1400, epoch: 9, batch: 48, loss: 1.80307, f1 score: 0.81832, speed: 1.42 step/s\r\n",
      "eval loss: 4.76699, f1 score: 0.43931, precison: 0.55207, recall: 0.39212\r\n",
      "eval loss: 4.24384, f1 score: 0.33172, precison: 0.29215, recall: 0.40612\r\n",
      "global step 1410, epoch: 9, batch: 58, loss: 1.56929, f1 score: 0.84637, speed: 0.50 step/s\r\n",
      "global step 1420, epoch: 9, batch: 68, loss: 1.57894, f1 score: 0.85160, speed: 1.45 step/s\r\n",
      "global step 1430, epoch: 9, batch: 78, loss: 1.44990, f1 score: 0.84243, speed: 1.52 step/s\r\n",
      "global step 1440, epoch: 9, batch: 88, loss: 1.35002, f1 score: 0.84098, speed: 1.61 step/s\r\n",
      "eval loss: 4.85277, f1 score: 0.43442, precison: 0.55719, recall: 0.38609\r\n",
      "eval loss: 4.34242, f1 score: 0.31387, precison: 0.30135, recall: 0.36488\r\n",
      "global step 1450, epoch: 9, batch: 98, loss: 1.75409, f1 score: 0.82707, speed: 0.49 step/s\r\n",
      "global step 1460, epoch: 9, batch: 108, loss: 1.72195, f1 score: 0.82263, speed: 1.55 step/s\r\n",
      "global step 1470, epoch: 9, batch: 118, loss: 2.28207, f1 score: 0.82270, speed: 1.45 step/s\r\n",
      "global step 1480, epoch: 9, batch: 128, loss: 1.51069, f1 score: 0.82234, speed: 1.51 step/s\r\n",
      "eval loss: 4.81368, f1 score: 0.45123, precison: 0.53910, recall: 0.41535\r\n",
      "eval loss: 4.35416, f1 score: 0.30666, precison: 0.27442, recall: 0.37532\r\n",
      "global step 1490, epoch: 9, batch: 138, loss: 1.79461, f1 score: 0.84394, speed: 0.49 step/s\r\n",
      "global step 1500, epoch: 9, batch: 148, loss: 1.46524, f1 score: 0.83215, speed: 1.44 step/s\r\n",
      "global step 1510, epoch: 9, batch: 158, loss: 1.78533, f1 score: 0.82601, speed: 1.41 step/s\r\n",
      "global step 1520, epoch: 9, batch: 168, loss: 1.78462, f1 score: 0.81998, speed: 1.57 step/s\r\n",
      "eval loss: 4.89945, f1 score: 0.43883, precison: 0.55409, recall: 0.39594\r\n",
      "eval loss: 4.35515, f1 score: 0.31712, precison: 0.29873, recall: 0.37831\r\n",
      "global step 1530, epoch: 10, batch: 9, loss: 1.21690, f1 score: 0.84811, speed: 0.49 step/s\r\n",
      "global step 1540, epoch: 10, batch: 19, loss: 1.59500, f1 score: 0.85172, speed: 1.46 step/s\r\n",
      "global step 1550, epoch: 10, batch: 29, loss: 1.27968, f1 score: 0.85927, speed: 1.51 step/s\r\n",
      "global step 1560, epoch: 10, batch: 39, loss: 1.54585, f1 score: 0.85910, speed: 1.37 step/s\r\n",
      "eval loss: 5.02626, f1 score: 0.44023, precison: 0.56223, recall: 0.39288\r\n",
      "eval loss: 4.33908, f1 score: 0.32158, precison: 0.30983, recall: 0.36320\r\n",
      "global step 1570, epoch: 10, batch: 49, loss: 1.27974, f1 score: 0.85585, speed: 0.49 step/s\r\n",
      "global step 1580, epoch: 10, batch: 59, loss: 1.12056, f1 score: 0.86246, speed: 1.51 step/s\r\n",
      "global step 1590, epoch: 10, batch: 69, loss: 1.15878, f1 score: 0.85705, speed: 1.35 step/s\r\n",
      "global step 1600, epoch: 10, batch: 79, loss: 1.34992, f1 score: 0.85580, speed: 1.47 step/s\r\n",
      "eval loss: 4.99854, f1 score: 0.44960, precison: 0.53704, recall: 0.40704\r\n",
      "eval loss: 4.66704, f1 score: 0.29785, precison: 0.29644, recall: 0.33845\r\n",
      "global step 1610, epoch: 10, batch: 89, loss: 1.75025, f1 score: 0.81825, speed: 0.49 step/s\r\n",
      "global step 1620, epoch: 10, batch: 99, loss: 1.32027, f1 score: 0.83713, speed: 1.48 step/s\r\n",
      "global step 1630, epoch: 10, batch: 109, loss: 1.33987, f1 score: 0.84403, speed: 1.52 step/s\r\n",
      "global step 1640, epoch: 10, batch: 119, loss: 1.58426, f1 score: 0.84766, speed: 1.52 step/s\r\n",
      "eval loss: 5.11832, f1 score: 0.44615, precison: 0.54373, recall: 0.40514\r\n",
      "eval loss: 4.57754, f1 score: 0.29613, precison: 0.28304, recall: 0.35232\r\n",
      "global step 1650, epoch: 10, batch: 129, loss: 1.55859, f1 score: 0.84047, speed: 0.50 step/s\r\n",
      "global step 1660, epoch: 10, batch: 139, loss: 1.12101, f1 score: 0.83626, speed: 1.54 step/s\r\n",
      "global step 1670, epoch: 10, batch: 149, loss: 1.35310, f1 score: 0.83062, speed: 1.39 step/s\r\n",
      "global step 1680, epoch: 10, batch: 159, loss: 1.23324, f1 score: 0.83530, speed: 1.36 step/s\r\n",
      "eval loss: 5.18012, f1 score: 0.44385, precison: 0.54543, recall: 0.40239\r\n",
      "eval loss: 4.57631, f1 score: 0.31389, precison: 0.30305, recall: 0.36366\r\n",
      "global step 1690, epoch: 10, batch: 169, loss: 1.88173, f1 score: 0.83644, speed: 0.49 step/s\r\n",
      "global step 1700, epoch: 11, batch: 10, loss: 0.91978, f1 score: 0.87382, speed: 1.45 step/s\r\n",
      "global step 1710, epoch: 11, batch: 20, loss: 1.03613, f1 score: 0.87674, speed: 1.48 step/s\r\n",
      "global step 1720, epoch: 11, batch: 30, loss: 1.14331, f1 score: 0.87608, speed: 1.48 step/s\r\n",
      "eval loss: 5.21270, f1 score: 0.43848, precison: 0.54087, recall: 0.39015\r\n",
      "eval loss: 4.83792, f1 score: 0.30766, precison: 0.30484, recall: 0.34402\r\n",
      "global step 1730, epoch: 11, batch: 40, loss: 1.07251, f1 score: 0.86091, speed: 0.51 step/s\r\n",
      "global step 1740, epoch: 11, batch: 50, loss: 1.41733, f1 score: 0.86805, speed: 1.36 step/s\r\n",
      "global step 1750, epoch: 11, batch: 60, loss: 1.44589, f1 score: 0.87108, speed: 1.47 step/s\r\n",
      "global step 1760, epoch: 11, batch: 70, loss: 1.07601, f1 score: 0.87524, speed: 1.48 step/s\r\n",
      "eval loss: 5.22746, f1 score: 0.45252, precison: 0.54263, recall: 0.40486\r\n",
      "eval loss: 4.93249, f1 score: 0.31408, precison: 0.30058, recall: 0.36260\r\n",
      "global step 1770, epoch: 11, batch: 80, loss: 1.14227, f1 score: 0.89621, speed: 0.50 step/s\r\n",
      "global step 1780, epoch: 11, batch: 90, loss: 1.51888, f1 score: 0.87401, speed: 1.37 step/s\r\n",
      "global step 1790, epoch: 11, batch: 100, loss: 1.49884, f1 score: 0.86549, speed: 1.44 step/s\r\n",
      "global step 1800, epoch: 11, batch: 110, loss: 1.22442, f1 score: 0.86935, speed: 1.51 step/s\r\n",
      "eval loss: 5.32435, f1 score: 0.44656, precison: 0.54881, recall: 0.40030\r\n",
      "eval loss: 4.89317, f1 score: 0.30097, precison: 0.29192, recall: 0.35002\r\n",
      "global step 1810, epoch: 11, batch: 120, loss: 1.01149, f1 score: 0.88826, speed: 0.48 step/s\r\n",
      "global step 1820, epoch: 11, batch: 130, loss: 1.20731, f1 score: 0.88248, speed: 1.38 step/s\r\n",
      "global step 1830, epoch: 11, batch: 140, loss: 1.48388, f1 score: 0.88161, speed: 1.58 step/s\r\n",
      "global step 1840, epoch: 11, batch: 150, loss: 1.21285, f1 score: 0.88072, speed: 1.52 step/s\r\n",
      "eval loss: 5.42895, f1 score: 0.43722, precison: 0.53318, recall: 0.40021\r\n",
      "eval loss: 4.95072, f1 score: 0.30905, precison: 0.29491, recall: 0.36784\r\n",
      "global step 1850, epoch: 11, batch: 160, loss: 1.18938, f1 score: 0.88864, speed: 0.51 step/s\r\n",
      "global step 1860, epoch: 12, batch: 1, loss: 1.47805, f1 score: 0.87664, speed: 1.51 step/s\r\n",
      "global step 1870, epoch: 12, batch: 11, loss: 0.96234, f1 score: 0.88172, speed: 1.56 step/s\r\n",
      "global step 1880, epoch: 12, batch: 21, loss: 1.02566, f1 score: 0.88539, speed: 1.40 step/s\r\n",
      "eval loss: 5.39214, f1 score: 0.44974, precison: 0.54188, recall: 0.40692\r\n",
      "eval loss: 4.84644, f1 score: 0.31152, precison: 0.30159, recall: 0.36223\r\n",
      "global step 1890, epoch: 12, batch: 31, loss: 1.05658, f1 score: 0.90052, speed: 0.50 step/s\r\n",
      "global step 1900, epoch: 12, batch: 41, loss: 1.05574, f1 score: 0.90406, speed: 1.40 step/s\r\n",
      "global step 1910, epoch: 12, batch: 51, loss: 1.10465, f1 score: 0.90487, speed: 1.53 step/s\r\n",
      "global step 1920, epoch: 12, batch: 61, loss: 1.27179, f1 score: 0.90384, speed: 1.42 step/s\r\n",
      "eval loss: 5.49041, f1 score: 0.44305, precison: 0.54908, recall: 0.39498\r\n",
      "eval loss: 5.21333, f1 score: 0.31428, precison: 0.31575, recall: 0.35254\r\n",
      "global step 1930, epoch: 12, batch: 71, loss: 1.27452, f1 score: 0.88996, speed: 0.48 step/s\r\n",
      "global step 1940, epoch: 12, batch: 81, loss: 1.18800, f1 score: 0.90658, speed: 1.56 step/s\r\n",
      "global step 1950, epoch: 12, batch: 91, loss: 1.33140, f1 score: 0.90206, speed: 1.56 step/s\r\n",
      "global step 1960, epoch: 12, batch: 101, loss: 1.15070, f1 score: 0.90798, speed: 1.34 step/s\r\n",
      "eval loss: 5.59478, f1 score: 0.44759, precison: 0.52776, recall: 0.41125\r\n",
      "eval loss: 5.08799, f1 score: 0.30481, precison: 0.29402, recall: 0.35708\r\n",
      "global step 1970, epoch: 12, batch: 111, loss: 0.98818, f1 score: 0.90097, speed: 0.49 step/s\r\n",
      "global step 1980, epoch: 12, batch: 121, loss: 0.77634, f1 score: 0.90086, speed: 1.43 step/s\r\n",
      "global step 1990, epoch: 12, batch: 131, loss: 0.81081, f1 score: 0.89415, speed: 1.47 step/s\r\n",
      "global step 2000, epoch: 12, batch: 141, loss: 1.18921, f1 score: 0.89469, speed: 1.64 step/s\r\n",
      "eval loss: 5.56300, f1 score: 0.44545, precison: 0.53991, recall: 0.40610\r\n",
      "eval loss: 5.12110, f1 score: 0.26225, precison: 0.22781, recall: 0.33256\r\n",
      "global step 2010, epoch: 12, batch: 151, loss: 1.09862, f1 score: 0.90160, speed: 0.49 step/s\r\n",
      "global step 2020, epoch: 12, batch: 161, loss: 1.03280, f1 score: 0.90509, speed: 1.49 step/s\r\n",
      "global step 2030, epoch: 13, batch: 2, loss: 1.07178, f1 score: 0.90644, speed: 1.47 step/s\r\n",
      "global step 2040, epoch: 13, batch: 12, loss: 0.69724, f1 score: 0.90922, speed: 1.47 step/s\r\n",
      "eval loss: 5.61915, f1 score: 0.44075, precison: 0.53808, recall: 0.39685\r\n",
      "eval loss: 5.11778, f1 score: 0.27055, precison: 0.24139, recall: 0.32933\r\n",
      "global step 2050, epoch: 13, batch: 22, loss: 1.15077, f1 score: 0.90192, speed: 0.51 step/s\r\n",
      "global step 2060, epoch: 13, batch: 32, loss: 1.04862, f1 score: 0.91572, speed: 1.47 step/s\r\n",
      "global step 2070, epoch: 13, batch: 42, loss: 0.82200, f1 score: 0.91753, speed: 1.41 step/s\r\n",
      "global step 2080, epoch: 13, batch: 52, loss: 1.00986, f1 score: 0.91970, speed: 1.44 step/s\r\n",
      "eval loss: 5.67553, f1 score: 0.44840, precison: 0.54493, recall: 0.40703\r\n",
      "eval loss: 5.29470, f1 score: 0.32104, precison: 0.34523, recall: 0.37081\r\n",
      "global step 2090, epoch: 13, batch: 62, loss: 0.90431, f1 score: 0.93282, speed: 0.50 step/s\r\n",
      "global step 2100, epoch: 13, batch: 72, loss: 1.04854, f1 score: 0.92231, speed: 1.39 step/s\r\n",
      "global step 2110, epoch: 13, batch: 82, loss: 1.22663, f1 score: 0.92916, speed: 1.42 step/s\r\n",
      "global step 2120, epoch: 13, batch: 92, loss: 0.92311, f1 score: 0.92986, speed: 1.51 step/s\r\n",
      "eval loss: 5.68907, f1 score: 0.45041, precison: 0.54388, recall: 0.40579\r\n",
      "eval loss: 5.21243, f1 score: 0.30508, precison: 0.29262, recall: 0.36155\r\n",
      "global step 2130, epoch: 13, batch: 102, loss: 0.72862, f1 score: 0.91687, speed: 0.48 step/s\r\n",
      "global step 2140, epoch: 13, batch: 112, loss: 1.13005, f1 score: 0.91710, speed: 1.56 step/s\r\n",
      "global step 2150, epoch: 13, batch: 122, loss: 0.97834, f1 score: 0.91766, speed: 1.39 step/s\r\n",
      "global step 2160, epoch: 13, batch: 132, loss: 0.99842, f1 score: 0.92072, speed: 1.47 step/s\r\n",
      "eval loss: 5.77360, f1 score: 0.44893, precison: 0.54073, recall: 0.40584\r\n",
      "eval loss: 5.14950, f1 score: 0.30615, precison: 0.29173, recall: 0.35920\r\n",
      "global step 2170, epoch: 13, batch: 142, loss: 1.10510, f1 score: 0.91407, speed: 0.50 step/s\r\n",
      "global step 2180, epoch: 13, batch: 152, loss: 0.87729, f1 score: 0.91806, speed: 1.61 step/s\r\n",
      "global step 2190, epoch: 13, batch: 162, loss: 1.22797, f1 score: 0.91398, speed: 1.39 step/s\r\n",
      "global step 2200, epoch: 14, batch: 3, loss: 1.00231, f1 score: 0.91712, speed: 1.46 step/s\r\n",
      "eval loss: 5.83963, f1 score: 0.44631, precison: 0.52728, recall: 0.41047\r\n",
      "eval loss: 5.14527, f1 score: 0.32284, precison: 0.33537, recall: 0.38229\r\n",
      "global step 2210, epoch: 14, batch: 13, loss: 0.80936, f1 score: 0.95115, speed: 0.50 step/s\r\n",
      "global step 2220, epoch: 14, batch: 23, loss: 0.82979, f1 score: 0.94901, speed: 1.51 step/s\r\n",
      "global step 2230, epoch: 14, batch: 33, loss: 0.78822, f1 score: 0.94324, speed: 1.47 step/s\r\n",
      "global step 2240, epoch: 14, batch: 43, loss: 0.98900, f1 score: 0.93950, speed: 1.47 step/s\r\n",
      "eval loss: 5.84140, f1 score: 0.44885, precison: 0.53869, recall: 0.40679\r\n",
      "eval loss: 5.27335, f1 score: 0.30457, precison: 0.29192, recall: 0.36008\r\n",
      "global step 2250, epoch: 14, batch: 53, loss: 0.74401, f1 score: 0.93496, speed: 0.50 step/s\r\n",
      "global step 2260, epoch: 14, batch: 63, loss: 0.91098, f1 score: 0.93441, speed: 1.55 step/s\r\n",
      "global step 2270, epoch: 14, batch: 73, loss: 1.00564, f1 score: 0.93395, speed: 1.62 step/s\r\n",
      "global step 2280, epoch: 14, batch: 83, loss: 0.84347, f1 score: 0.93414, speed: 1.52 step/s\r\n",
      "eval loss: 5.90902, f1 score: 0.45272, precison: 0.53521, recall: 0.41485\r\n",
      "eval loss: 5.32185, f1 score: 0.31182, precison: 0.29150, recall: 0.37528\r\n",
      "global step 2290, epoch: 14, batch: 93, loss: 0.47796, f1 score: 0.93496, speed: 0.50 step/s\r\n",
      "global step 2300, epoch: 14, batch: 103, loss: 0.95938, f1 score: 0.92955, speed: 1.57 step/s\r\n",
      "global step 2310, epoch: 14, batch: 113, loss: 1.07302, f1 score: 0.92786, speed: 1.42 step/s\r\n",
      "global step 2320, epoch: 14, batch: 123, loss: 0.77073, f1 score: 0.93374, speed: 1.50 step/s\r\n",
      "eval loss: 5.94732, f1 score: 0.45271, precison: 0.54225, recall: 0.41577\r\n",
      "eval loss: 5.41170, f1 score: 0.31085, precison: 0.29388, recall: 0.36543\r\n",
      "global step 2330, epoch: 14, batch: 133, loss: 0.75554, f1 score: 0.94113, speed: 0.50 step/s\r\n",
      "global step 2340, epoch: 14, batch: 143, loss: 0.92108, f1 score: 0.93860, speed: 1.32 step/s\r\n",
      "global step 2350, epoch: 14, batch: 153, loss: 1.03041, f1 score: 0.93325, speed: 1.45 step/s\r\n",
      "global step 2360, epoch: 14, batch: 163, loss: 0.72538, f1 score: 0.93556, speed: 1.42 step/s\r\n",
      "eval loss: 5.96142, f1 score: 0.44376, precison: 0.54629, recall: 0.40012\r\n",
      "eval loss: 5.27334, f1 score: 0.27982, precison: 0.24741, recall: 0.34387\r\n",
      "global step 2370, epoch: 15, batch: 4, loss: 0.91572, f1 score: 0.94796, speed: 0.48 step/s\r\n",
      "global step 2380, epoch: 15, batch: 14, loss: 0.77980, f1 score: 0.95383, speed: 1.51 step/s\r\n",
      "global step 2390, epoch: 15, batch: 24, loss: 0.74765, f1 score: 0.94939, speed: 1.45 step/s\r\n",
      "global step 2400, epoch: 15, batch: 34, loss: 0.70759, f1 score: 0.94488, speed: 1.44 step/s\r\n",
      "eval loss: 5.98330, f1 score: 0.44984, precison: 0.53677, recall: 0.40880\r\n",
      "eval loss: 5.37312, f1 score: 0.31363, precison: 0.29843, recall: 0.37175\r\n",
      "global step 2410, epoch: 15, batch: 44, loss: 1.03989, f1 score: 0.94426, speed: 0.52 step/s\r\n",
      "global step 2420, epoch: 15, batch: 54, loss: 0.62751, f1 score: 0.93979, speed: 1.51 step/s\r\n",
      "global step 2430, epoch: 15, batch: 64, loss: 0.59458, f1 score: 0.94347, speed: 1.51 step/s\r\n",
      "global step 2440, epoch: 15, batch: 74, loss: 0.69858, f1 score: 0.94674, speed: 1.66 step/s\r\n",
      "eval loss: 6.02721, f1 score: 0.45079, precison: 0.53538, recall: 0.41122\r\n",
      "eval loss: 5.50618, f1 score: 0.29770, precison: 0.28381, recall: 0.34828\r\n",
      "global step 2450, epoch: 15, batch: 84, loss: 0.62568, f1 score: 0.94468, speed: 0.50 step/s\r\n",
      "global step 2460, epoch: 15, batch: 94, loss: 0.70823, f1 score: 0.94828, speed: 1.41 step/s\r\n",
      "global step 2470, epoch: 15, batch: 104, loss: 0.83705, f1 score: 0.94625, speed: 1.45 step/s\r\n",
      "global step 2480, epoch: 15, batch: 114, loss: 0.81740, f1 score: 0.94363, speed: 1.40 step/s\r\n",
      "eval loss: 6.09999, f1 score: 0.44576, precison: 0.53197, recall: 0.40992\r\n",
      "eval loss: 5.44618, f1 score: 0.31879, precison: 0.29497, recall: 0.38935\r\n",
      "global step 2490, epoch: 15, batch: 124, loss: 0.70598, f1 score: 0.94793, speed: 0.49 step/s\r\n",
      "global step 2500, epoch: 15, batch: 134, loss: 0.67993, f1 score: 0.94067, speed: 1.47 step/s\r\n",
      "global step 2510, epoch: 15, batch: 144, loss: 0.83407, f1 score: 0.95065, speed: 1.39 step/s\r\n",
      "global step 2520, epoch: 15, batch: 154, loss: 0.64344, f1 score: 0.95053, speed: 1.51 step/s\r\n",
      "eval loss: 6.04755, f1 score: 0.44922, precison: 0.53947, recall: 0.40476\r\n",
      "eval loss: 5.57525, f1 score: 0.30753, precison: 0.29536, recall: 0.36283\r\n",
      "global step 2530, epoch: 15, batch: 164, loss: 0.81003, f1 score: 0.95700, speed: 0.49 step/s\r\n",
      "global step 2540, epoch: 16, batch: 5, loss: 0.85728, f1 score: 0.94530, speed: 1.60 step/s\r\n",
      "global step 2550, epoch: 16, batch: 15, loss: 0.77094, f1 score: 0.95236, speed: 1.62 step/s\r\n",
      "global step 2560, epoch: 16, batch: 25, loss: 0.48771, f1 score: 0.95104, speed: 1.48 step/s\r\n",
      "eval loss: 6.10200, f1 score: 0.44861, precison: 0.54850, recall: 0.40300\r\n",
      "eval loss: 5.59535, f1 score: 0.30978, precison: 0.29622, recall: 0.36565\r\n",
      "global step 2570, epoch: 16, batch: 35, loss: 0.60927, f1 score: 0.94242, speed: 0.48 step/s\r\n",
      "global step 2580, epoch: 16, batch: 45, loss: 1.11384, f1 score: 0.95546, speed: 1.51 step/s\r\n",
      "global step 2590, epoch: 16, batch: 55, loss: 0.54875, f1 score: 0.95840, speed: 1.51 step/s\r\n",
      "global step 2600, epoch: 16, batch: 65, loss: 0.77647, f1 score: 0.95601, speed: 1.54 step/s\r\n",
      "eval loss: 6.09529, f1 score: 0.44632, precison: 0.54461, recall: 0.40161\r\n",
      "eval loss: 5.65788, f1 score: 0.26749, precison: 0.24168, recall: 0.32607\r\n",
      "global step 2610, epoch: 16, batch: 75, loss: 0.68151, f1 score: 0.95443, speed: 0.50 step/s\r\n",
      "global step 2620, epoch: 16, batch: 85, loss: 0.59063, f1 score: 0.95828, speed: 1.53 step/s\r\n",
      "global step 2630, epoch: 16, batch: 95, loss: 0.61165, f1 score: 0.96127, speed: 1.51 step/s\r\n",
      "global step 2640, epoch: 16, batch: 105, loss: 0.60683, f1 score: 0.95905, speed: 1.40 step/s\r\n",
      "eval loss: 6.16340, f1 score: 0.44930, precison: 0.54122, recall: 0.40769\r\n",
      "eval loss: 5.62267, f1 score: 0.30440, precison: 0.29279, recall: 0.35597\r\n",
      "global step 2650, epoch: 16, batch: 115, loss: 0.71223, f1 score: 0.95554, speed: 0.49 step/s\r\n",
      "global step 2660, epoch: 16, batch: 125, loss: 0.68301, f1 score: 0.95777, speed: 1.57 step/s\r\n",
      "global step 2670, epoch: 16, batch: 135, loss: 0.66364, f1 score: 0.95230, speed: 1.41 step/s\r\n",
      "global step 2680, epoch: 16, batch: 145, loss: 0.82715, f1 score: 0.95541, speed: 1.38 step/s\r\n",
      "eval loss: 6.23427, f1 score: 0.44977, precison: 0.54217, recall: 0.41214\r\n",
      "eval loss: 5.70320, f1 score: 0.30616, precison: 0.29193, recall: 0.36493\r\n",
      "global step 2690, epoch: 16, batch: 155, loss: 0.57245, f1 score: 0.94448, speed: 0.48 step/s\r\n",
      "global step 2700, epoch: 16, batch: 165, loss: 0.52862, f1 score: 0.95380, speed: 1.47 step/s\r\n",
      "global step 2710, epoch: 17, batch: 6, loss: 0.68517, f1 score: 0.95669, speed: 1.49 step/s\r\n",
      "global step 2720, epoch: 17, batch: 16, loss: 0.59516, f1 score: 0.96046, speed: 1.58 step/s\r\n",
      "eval loss: 6.21313, f1 score: 0.45083, precison: 0.54727, recall: 0.40980\r\n",
      "eval loss: 5.59324, f1 score: 0.29934, precison: 0.28468, recall: 0.35397\r\n",
      "global step 2730, epoch: 17, batch: 26, loss: 0.68407, f1 score: 0.96115, speed: 0.50 step/s\r\n",
      "global step 2740, epoch: 17, batch: 36, loss: 0.56098, f1 score: 0.95935, speed: 1.60 step/s\r\n",
      "global step 2750, epoch: 17, batch: 46, loss: 0.53410, f1 score: 0.96216, speed: 1.46 step/s\r\n",
      "global step 2760, epoch: 17, batch: 56, loss: 0.62438, f1 score: 0.95904, speed: 1.42 step/s\r\n",
      "eval loss: 6.20152, f1 score: 0.44907, precison: 0.55194, recall: 0.40404\r\n",
      "eval loss: 5.66076, f1 score: 0.31032, precison: 0.29471, recall: 0.37093\r\n",
      "global step 2770, epoch: 17, batch: 66, loss: 0.58393, f1 score: 0.95810, speed: 0.49 step/s\r\n",
      "global step 2780, epoch: 17, batch: 76, loss: 0.71907, f1 score: 0.96000, speed: 1.54 step/s\r\n",
      "global step 2790, epoch: 17, batch: 86, loss: 0.78887, f1 score: 0.96041, speed: 1.42 step/s\r\n",
      "global step 2800, epoch: 17, batch: 96, loss: 0.73676, f1 score: 0.96221, speed: 1.61 step/s\r\n",
      "eval loss: 6.24164, f1 score: 0.44907, precison: 0.54441, recall: 0.40702\r\n",
      "eval loss: 5.73917, f1 score: 0.29926, precison: 0.28589, recall: 0.35432\r\n",
      "global step 2810, epoch: 17, batch: 106, loss: 0.73019, f1 score: 0.96000, speed: 0.49 step/s\r\n",
      "global step 2820, epoch: 17, batch: 116, loss: 0.62472, f1 score: 0.95649, speed: 1.49 step/s\r\n",
      "global step 2830, epoch: 17, batch: 126, loss: 0.66419, f1 score: 0.95684, speed: 1.39 step/s\r\n",
      "global step 2840, epoch: 17, batch: 136, loss: 0.64141, f1 score: 0.95746, speed: 1.43 step/s\r\n",
      "eval loss: 6.27692, f1 score: 0.45142, precison: 0.54990, recall: 0.40843\r\n",
      "eval loss: 5.72623, f1 score: 0.30320, precison: 0.28841, recall: 0.35795\r\n",
      "global step 2850, epoch: 17, batch: 146, loss: 0.90291, f1 score: 0.95644, speed: 0.49 step/s\r\n",
      "global step 2860, epoch: 17, batch: 156, loss: 0.46571, f1 score: 0.95897, speed: 1.31 step/s\r\n",
      "global step 2870, epoch: 17, batch: 166, loss: 0.64054, f1 score: 0.95643, speed: 1.46 step/s\r\n",
      "global step 2880, epoch: 18, batch: 7, loss: 0.61345, f1 score: 0.95858, speed: 1.50 step/s\r\n",
      "eval loss: 6.28145, f1 score: 0.44929, precison: 0.54337, recall: 0.40612\r\n",
      "eval loss: 5.72062, f1 score: 0.30515, precison: 0.29034, recall: 0.36103\r\n",
      "global step 2890, epoch: 18, batch: 17, loss: 0.56862, f1 score: 0.96141, speed: 0.50 step/s\r\n",
      "global step 2900, epoch: 18, batch: 27, loss: 0.42827, f1 score: 0.96359, speed: 1.46 step/s\r\n",
      "global step 2910, epoch: 18, batch: 37, loss: 0.62910, f1 score: 0.96395, speed: 1.49 step/s\r\n",
      "global step 2920, epoch: 18, batch: 47, loss: 0.91030, f1 score: 0.96736, speed: 1.46 step/s\r\n",
      "eval loss: 6.30748, f1 score: 0.44706, precison: 0.54353, recall: 0.40325\r\n",
      "eval loss: 5.72359, f1 score: 0.30492, precison: 0.29205, recall: 0.35682\r\n",
      "global step 2930, epoch: 18, batch: 57, loss: 0.78111, f1 score: 0.96354, speed: 0.50 step/s\r\n",
      "global step 2940, epoch: 18, batch: 67, loss: 0.71280, f1 score: 0.96025, speed: 1.49 step/s\r\n",
      "global step 2950, epoch: 18, batch: 77, loss: 0.48987, f1 score: 0.96406, speed: 1.47 step/s\r\n",
      "global step 2960, epoch: 18, batch: 87, loss: 0.55295, f1 score: 0.96679, speed: 1.67 step/s\r\n",
      "eval loss: 6.32958, f1 score: 0.45126, precison: 0.54413, recall: 0.41049\r\n",
      "eval loss: 5.80886, f1 score: 0.30305, precison: 0.28785, recall: 0.35838\r\n",
      "global step 2970, epoch: 18, batch: 97, loss: 0.28783, f1 score: 0.94973, speed: 0.50 step/s\r\n",
      "global step 2980, epoch: 18, batch: 107, loss: 0.52254, f1 score: 0.96402, speed: 1.43 step/s\r\n",
      "global step 2990, epoch: 18, batch: 117, loss: 0.66562, f1 score: 0.96515, speed: 1.45 step/s\r\n",
      "global step 3000, epoch: 18, batch: 127, loss: 0.46761, f1 score: 0.96674, speed: 1.48 step/s\r\n",
      "eval loss: 6.33851, f1 score: 0.45125, precison: 0.54357, recall: 0.40850\r\n",
      "eval loss: 5.76522, f1 score: 0.30329, precison: 0.29257, recall: 0.35150\r\n",
      "global step 3010, epoch: 18, batch: 137, loss: 0.53072, f1 score: 0.96773, speed: 0.49 step/s\r\n",
      "global step 3020, epoch: 18, batch: 147, loss: 0.48555, f1 score: 0.96945, speed: 1.48 step/s\r\n",
      "global step 3030, epoch: 18, batch: 157, loss: 0.30436, f1 score: 0.96710, speed: 1.38 step/s\r\n",
      "global step 3040, epoch: 18, batch: 167, loss: 0.51632, f1 score: 0.96933, speed: 1.59 step/s\r\n",
      "eval loss: 6.37937, f1 score: 0.45334, precison: 0.54576, recall: 0.40965\r\n",
      "eval loss: 5.69121, f1 score: 0.31063, precison: 0.30001, recall: 0.35761\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-17 19:30:14,063] [    INFO] - tokenizer config file saved in Bert_Bilstm_ckpt/tokenizer_config.json\r\n",
      "[2023-01-17 19:30:14,068] [    INFO] - Special tokens file saved in Bert_Bilstm_ckpt/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3050, epoch: 19, batch: 8, loss: 0.74896, f1 score: 0.94962, speed: 0.17 step/s\r\n",
      "global step 3060, epoch: 19, batch: 18, loss: 0.44952, f1 score: 0.95900, speed: 1.71 step/s\r\n",
      "global step 3070, epoch: 19, batch: 28, loss: 0.65855, f1 score: 0.95804, speed: 1.50 step/s\r\n",
      "global step 3080, epoch: 19, batch: 38, loss: 0.67777, f1 score: 0.96008, speed: 1.44 step/s\r\n",
      "eval loss: 6.36614, f1 score: 0.45142, precison: 0.53887, recall: 0.40893\r\n",
      "eval loss: 5.77169, f1 score: 0.30314, precison: 0.29103, recall: 0.35441\r\n",
      "global step 3090, epoch: 19, batch: 48, loss: 0.45143, f1 score: 0.97182, speed: 0.50 step/s\r\n",
      "global step 3100, epoch: 19, batch: 58, loss: 0.57301, f1 score: 0.96708, speed: 1.52 step/s\r\n",
      "global step 3110, epoch: 19, batch: 68, loss: 0.58417, f1 score: 0.96852, speed: 1.34 step/s\r\n",
      "global step 3120, epoch: 19, batch: 78, loss: 0.88021, f1 score: 0.96889, speed: 1.52 step/s\r\n",
      "eval loss: 6.39479, f1 score: 0.45082, precison: 0.53466, recall: 0.41155\r\n",
      "eval loss: 5.78833, f1 score: 0.30875, precison: 0.29393, recall: 0.36429\r\n",
      "global step 3130, epoch: 19, batch: 88, loss: 0.55842, f1 score: 0.97269, speed: 0.51 step/s\r\n",
      "global step 3140, epoch: 19, batch: 98, loss: 0.56143, f1 score: 0.97675, speed: 1.37 step/s\r\n",
      "global step 3150, epoch: 19, batch: 108, loss: 0.57869, f1 score: 0.97282, speed: 1.43 step/s\r\n",
      "global step 3160, epoch: 19, batch: 118, loss: 0.62835, f1 score: 0.96954, speed: 1.49 step/s\r\n",
      "eval loss: 6.38675, f1 score: 0.45184, precison: 0.54521, recall: 0.40857\r\n",
      "eval loss: 5.77005, f1 score: 0.30995, precison: 0.30016, recall: 0.36023\r\n",
      "global step 3170, epoch: 19, batch: 128, loss: 0.49910, f1 score: 0.97669, speed: 0.50 step/s\r\n",
      "global step 3180, epoch: 19, batch: 138, loss: 0.71593, f1 score: 0.97572, speed: 1.65 step/s\r\n",
      "global step 3190, epoch: 19, batch: 148, loss: 0.56260, f1 score: 0.97773, speed: 1.45 step/s\r\n",
      "global step 3200, epoch: 19, batch: 158, loss: 0.46280, f1 score: 0.97352, speed: 1.45 step/s\r\n",
      "eval loss: 6.37763, f1 score: 0.45266, precison: 0.54492, recall: 0.40841\r\n",
      "eval loss: 5.77933, f1 score: 0.31019, precison: 0.29624, recall: 0.36683\r\n",
      "global step 3210, epoch: 19, batch: 168, loss: 0.47305, f1 score: 0.98128, speed: 0.49 step/s\r\n",
      "global step 3220, epoch: 20, batch: 9, loss: 0.41343, f1 score: 0.98122, speed: 1.52 step/s\r\n",
      "global step 3230, epoch: 20, batch: 19, loss: 0.57244, f1 score: 0.97419, speed: 1.54 step/s\r\n",
      "global step 3240, epoch: 20, batch: 29, loss: 0.55160, f1 score: 0.97381, speed: 1.38 step/s\r\n",
      "eval loss: 6.38829, f1 score: 0.45095, precison: 0.53965, recall: 0.40852\r\n",
      "eval loss: 5.79796, f1 score: 0.30995, precison: 0.29418, recall: 0.36600\r\n",
      "global step 3250, epoch: 20, batch: 39, loss: 0.50070, f1 score: 0.97351, speed: 0.49 step/s\r\n",
      "global step 3260, epoch: 20, batch: 49, loss: 0.36701, f1 score: 0.97380, speed: 1.38 step/s\r\n",
      "global step 3270, epoch: 20, batch: 59, loss: 0.52669, f1 score: 0.97291, speed: 1.56 step/s\r\n",
      "global step 3280, epoch: 20, batch: 69, loss: 0.54877, f1 score: 0.97406, speed: 1.49 step/s\r\n",
      "eval loss: 6.39606, f1 score: 0.45085, precison: 0.54224, recall: 0.40765\r\n",
      "eval loss: 5.80144, f1 score: 0.30556, precison: 0.29230, recall: 0.35766\r\n",
      "global step 3290, epoch: 20, batch: 79, loss: 0.61251, f1 score: 0.96488, speed: 0.49 step/s\r\n",
      "global step 3300, epoch: 20, batch: 89, loss: 0.56482, f1 score: 0.96327, speed: 1.44 step/s\r\n",
      "global step 3310, epoch: 20, batch: 99, loss: 0.42801, f1 score: 0.96444, speed: 1.52 step/s\r\n",
      "global step 3320, epoch: 20, batch: 109, loss: 0.39214, f1 score: 0.96640, speed: 1.38 step/s\r\n",
      "eval loss: 6.40138, f1 score: 0.44978, precison: 0.53965, recall: 0.40681\r\n",
      "eval loss: 5.81363, f1 score: 0.30602, precison: 0.29215, recall: 0.35973\r\n",
      "global step 3330, epoch: 20, batch: 119, loss: 0.82923, f1 score: 0.96709, speed: 0.50 step/s\r\n",
      "global step 3340, epoch: 20, batch: 129, loss: 0.50271, f1 score: 0.97337, speed: 1.42 step/s\r\n",
      "global step 3350, epoch: 20, batch: 139, loss: 0.48753, f1 score: 0.97491, speed: 1.43 step/s\r\n",
      "global step 3360, epoch: 20, batch: 149, loss: 0.68718, f1 score: 0.97591, speed: 1.43 step/s\r\n",
      "eval loss: 6.40357, f1 score: 0.45068, precison: 0.53919, recall: 0.40822\r\n",
      "eval loss: 5.81233, f1 score: 0.30880, precison: 0.29376, recall: 0.36404\r\n",
      "global step 3370, epoch: 20, batch: 159, loss: 0.50978, f1 score: 0.98235, speed: 0.50 step/s\r\n",
      "global step 3380, epoch: 20, batch: 169, loss: 0.65984, f1 score: 0.97229, speed: 1.49 step/s\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ckpt_dir = \"Bert_Bilstm_ckpt\" # 训练过程中保存模型参数的文件夹\n",
    "\n",
    "global_step = 0  # 迭代次数\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "best_f1_score2 = 0\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(1, config.EPOCHS + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率、f1分数\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = logits\n",
    "        metric.update(probs, labels)\n",
    "        \n",
    "        # auc, f1_score, _,  _= metric.accumulate()\n",
    "        f1_score, _,  _= metric.accumulate()\n",
    "\n",
    "\n",
    "        # 每迭代100次，打印损失函数值、准确率、f1分数、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        #每迭代40次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
    "        if global_step % 40 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, valid_data_loader, label_vocab, if_return_results=False)\n",
    "            eval_f1_score2 = evaluate(model, criterion, metric, validZhihu_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                paddle.save(model.state_dict(), \"model_net.pdparams\")\n",
    "                paddle.save(optimizer.state_dict(), \"optimizer.pdopt\")\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:41:11.555193Z",
     "iopub.status.busy": "2023-01-17T11:41:11.554225Z",
     "iopub.status.idle": "2023-01-17T11:41:27.139277Z",
     "shell.execute_reply": "2023-01-17T11:41:27.138429Z",
     "shell.execute_reply.started": "2023-01-17T11:41:11.555150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0 在20分类验证集的最佳表现： eval loss: 6.37937, f1 score: 0.45334, precison: 0.54576, recall: 0.40965\r\n",
      "ERNIE 2.0 在20分类知乎验证集的最佳表现： eval loss: 5.69121, f1 score: 0.31063, precison: 0.30001, recall: 0.35761\r\n"
     ]
    }
   ],
   "source": [
    "# 模型加载\r\n",
    "# 加载验证集上效果最好的模型参数\r\n",
    "model.set_dict(paddle.load('model_net.pdparams'))\r\n",
    "\r\n",
    "# 加载之前训练好的模型参数\r\n",
    "# model.set_dict(paddle.load('/home/aistudio/work/model_state.pdparams'))\r\n",
    "\r\n",
    "# 模型验证\r\n",
    "print(\"ERNIE 3.0 在20分类验证集的最佳表现：\", end= \" \")\r\n",
    "results1 = evaluate(model, criterion, metric, valid_data_loader, label_vocab)\r\n",
    "print(\"ERNIE 2.0 在20分类知乎验证集的最佳表现：\", end= \" \")\r\n",
    "results = evaluate(model, criterion, metric, validZhihu_data_loader, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:42:25.792016Z",
     "iopub.status.busy": "2023-01-17T11:42:25.791371Z",
     "iopub.status.idle": "2023-01-17T11:42:25.800590Z",
     "shell.execute_reply": "2023-01-17T11:42:25.799746Z",
     "shell.execute_reply.started": "2023-01-17T11:42:25.791973Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def result2tsv(result1,filepath):\r\n",
    "    validData = pd.read_csv(filepath,sep='\\t')\r\n",
    "    dictvalidT =validData.to_dict(\"list\")\r\n",
    "\r\n",
    "    validPred = {}\r\n",
    "    validPred[\"Argument ID\"] = dictvalidT[\"Argument ID\"]\r\n",
    "    validPred[\"sentence\"] = dictvalidT[\"sentence\"]\r\n",
    "\r\n",
    "    for x in label_vocab:\r\n",
    "        validPred[x] = []\r\n",
    "    \r\n",
    "    for x in range(len(results1)):\r\n",
    "        types = results1[x].split(\",\")\r\n",
    "        if types == ['']:\r\n",
    "            for y in label_vocab:\r\n",
    "                validPred[y].append(0)\r\n",
    "        else:  \r\n",
    "            for z in label_vocab:\r\n",
    "                if z in types:\r\n",
    "                    validPred[z].append(1)\r\n",
    "                else:\r\n",
    "                    validPred[z].append(0)\r\n",
    "    validData = pd.read_csv(filepath,sep='\\t')\r\n",
    "    for x in label_vocab:\r\n",
    "        for y in range(len(validData[x])):\r\n",
    "            validData[x].iloc[y] = validPred[x][y]\r\n",
    "    \r\n",
    "    validData.drop(columns=[\"sentence\"],inplace=True)\r\n",
    "    return validData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:42:28.375733Z",
     "iopub.status.busy": "2023-01-17T11:42:28.375122Z",
     "iopub.status.idle": "2023-01-17T11:42:28.383684Z",
     "shell.execute_reply": "2023-01-17T11:42:28.382581Z",
     "shell.execute_reply.started": "2023-01-17T11:42:28.375692Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data\r\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:42:45.570636Z",
     "iopub.status.busy": "2023-01-17T11:42:45.569848Z",
     "iopub.status.idle": "2023-01-17T11:42:49.524702Z",
     "shell.execute_reply": "2023-01-17T11:42:49.523819Z",
     "shell.execute_reply.started": "2023-01-17T11:42:45.570584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "valid = result2tsv(results1,\"validation.tsv\")\r\n",
    "valid.to_csv('validbert.tsv',columns=valid.columns.tolist(),\r\n",
    "            sep='\\t',\r\n",
    "            index=False)\r\n",
    "\r\n",
    "validzhihu = result2tsv(results,\"zhihu_validation.tsv\")\r\n",
    "validzhihu.to_csv('validbertzhihu.tsv',columns=validzhihu.columns.tolist(),\r\n",
    "            sep='\\t',\r\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:44:19.279230Z",
     "iopub.status.busy": "2023-01-17T11:44:19.278462Z",
     "iopub.status.idle": "2023-01-17T11:44:19.286467Z",
     "shell.execute_reply": "2023-01-17T11:44:19.285462Z",
     "shell.execute_reply.started": "2023-01-17T11:44:19.279170Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预测函数，对测试集结果进行预测\r\n",
    "def predict(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\r\n",
    "    model.eval()\r\n",
    "    results = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        probs = logits\r\n",
    "        if if_return_results:\r\n",
    "            probs = probs.tolist()\r\n",
    "            for prob in probs:\r\n",
    "                result = []\r\n",
    "                for c, pred in enumerate(prob):\r\n",
    "                    if pred > 0:\r\n",
    "                        result.append(label_vocab[c])\r\n",
    "                        # result.append(str(c))\r\n",
    "                results.append(','.join(result))\r\n",
    "    if if_return_results:\r\n",
    "        return results\r\n",
    "    else:\r\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:43:13.622859Z",
     "iopub.status.busy": "2023-01-17T11:43:13.622199Z",
     "iopub.status.idle": "2023-01-17T11:43:13.630653Z",
     "shell.execute_reply": "2023-01-17T11:43:13.629649Z",
     "shell.execute_reply.started": "2023-01-17T11:43:13.622816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testOutput(results2,filepath):\r\n",
    "    testData = pd.read_csv(filepath,sep='\\t')\r\n",
    "    dicttestT =testData.to_dict(\"list\")\r\n",
    "\r\n",
    "    testPred = {}\r\n",
    "    testPred[\"Argument ID\"] = dicttestT[\"Argument ID\"]\r\n",
    "    testPred[\"sentence\"] = dicttestT[\"sentence\"]\r\n",
    "\r\n",
    "    for x in label_vocab:\r\n",
    "        testPred[x] = []\r\n",
    "\r\n",
    "    for x in range(len(results2)):\r\n",
    "        types = results2[x].split(\",\")\r\n",
    "        if types == ['']:\r\n",
    "            for y in label_vocab:\r\n",
    "                testPred[y].append(0)\r\n",
    "        else:  \r\n",
    "            for z in label_vocab:\r\n",
    "                if z in types:\r\n",
    "                    testPred[z].append(1)\r\n",
    "                else:\r\n",
    "                    testPred[z].append(0)\r\n",
    "    testPredD = pd.DataFrame.from_dict(testPred)\r\n",
    "    testPredD.drop(columns=[\"sentence\"],inplace=True)\r\n",
    "\r\n",
    "    return testPredD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:44:22.873563Z",
     "iopub.status.busy": "2023-01-17T11:44:22.872236Z",
     "iopub.status.idle": "2023-01-17T11:44:33.290058Z",
     "shell.execute_reply": "2023-01-17T11:44:33.288819Z",
     "shell.execute_reply.started": "2023-01-17T11:44:22.873517Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results2 = predict(model, criterion, metric, test_data_loader, label_vocab)\r\n",
    "test = testOutput(results2,\"test.tsv\")\r\n",
    "test.to_csv('testBert.tsv',columns=test.columns.tolist(),\r\n",
    "            sep='\\t',\r\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T11:50:25.909017Z",
     "iopub.status.busy": "2023-01-17T11:50:25.908230Z",
     "iopub.status.idle": "2023-01-17T11:50:27.896196Z",
     "shell.execute_reply": "2023-01-17T11:50:27.895149Z",
     "shell.execute_reply.started": "2023-01-17T11:50:25.908973Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results3 = predict(model, criterion, metric, test2_data_loader, label_vocab)\r\n",
    "test2 = testOutput(results3,\"test2.tsv\")\r\n",
    "test2.to_csv('test222Bert.tsv',columns=test2.columns.tolist(),\r\n",
    "            sep='\\t',\r\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
